[
  {
    "objectID": "notebooks/script.html",
    "href": "notebooks/script.html",
    "title": "script",
    "section": "",
    "text": "``\nBetter CSV Storage\nFor User/Programmers who don’t want to use databases and want to use csv instead, this script will help manage that csv file (storage)\nWhat can you do? - Filter data based on certain conditions - Update certain values in particular row\nHow to use? - It’s OOP based so just import it into your script\n\n\nLet’s say I want to check and download the status of the files. I can run a script like this:\ncsv_path = 'dummy_data.csv'\n\n#init class and get the object\ncsv_obj = BetterCSVStorage(csv_path)\n\n#Now user that object as storage\n\n#Filter Data Based on Different parameters\n#This List will contain additional ( update_index ) which is row index of each row in csv.\n#This update_index will be use to update value of certain index.\nfiltered_data = csv_obj.get_filtered_data('download_status', '==', '')\n\n#Change Data Based on Different parameters\ncsv_obj.update_data(10,'download_status', 'done')",
    "crumbs": [
      "Reference",
      "Notebooks",
      "script"
    ]
  },
  {
    "objectID": "notebooks/script.html#examples",
    "href": "notebooks/script.html#examples",
    "title": "script",
    "section": "",
    "text": "Let’s say I want to check and download the status of the files. I can run a script like this:\ncsv_path = 'dummy_data.csv'\n\n#init class and get the object\ncsv_obj = BetterCSVStorage(csv_path)\n\n#Now user that object as storage\n\n#Filter Data Based on Different parameters\n#This List will contain additional ( update_index ) which is row index of each row in csv.\n#This update_index will be use to update value of certain index.\nfiltered_data = csv_obj.get_filtered_data('download_status', '==', '')\n\n#Change Data Based on Different parameters\ncsv_obj.update_data(10,'download_status', 'done')",
    "crumbs": [
      "Reference",
      "Notebooks",
      "script"
    ]
  },
  {
    "objectID": "notebooks/group_by.html",
    "href": "notebooks/group_by.html",
    "title": "group_by",
    "section": "",
    "text": "``\n\n\n\n\n\nName\nDescription\n\n\n\n\nDynamicGroupBy\nA dynamic grouper.\n\n\nGroupBy\nStarts a new GroupBy operation.\n\n\nRollingGroupBy\nA rolling grouper.\n\n\n\n\n\nDynamicGroupBy(self, df, index_column, *, every, period, offset, include_boundaries, closed, label, group_by, start_by)\nA dynamic grouper.\nThis has an .agg method which allows you to run all polars expressions in a group by context.\n\n\n\n\n\nName\nDescription\n\n\n\n\nagg\nCompute aggregations for each group of a group by operation.\n\n\nmap_groups\nApply a custom/user-defined function (UDF) over the groups as a new DataFrame.\n\n\n\n\n\nDynamicGroupBy.agg(*aggs, **named_aggs)\nCompute aggregations for each group of a group by operation.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n*aggs\nIntoExpr | Iterable[IntoExpr]\nAggregations to compute for each group of the group by operation, specified as positional arguments. Accepts expression input. Strings are parsed as column names.\n()\n\n\n**named_aggs\nIntoExpr\nAdditional aggregations, specified as keyword arguments. The resulting columns will be renamed to the keyword used.\n{}\n\n\n\n\n\n\n\nDynamicGroupBy.map_groups(function, schema)\nApply a custom/user-defined function (UDF) over the groups as a new DataFrame.\nUsing this is considered an anti-pattern as it will be very slow because:\n\nit forces the engine to materialize the whole DataFrames for the groups.\nit is not parallelized.\nit blocks optimizations as the passed python function is opaque to the optimizer.\n\nThe idiomatic way to apply custom functions over multiple columns is using:\npl.struct([my_columns]).map_elements(lambda struct_series: ..)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunction\nCallable[[DataFrame], DataFrame]\nFunction to apply over each group of the LazyFrame; it receives a DataFrame and should return a DataFrame.\nrequired\n\n\nschema\nSchemaDict | None\nSchema of the output function. This has to be known statically. If the given schema is incorrect, this is a bug in the caller’s query and may lead to errors. If set to None, polars assumes the schema is unchanged.\nrequired\n\n\n\n\n\n\n\n\n\nGroupBy(self, df, *by, maintain_order, **named_by)\nStarts a new GroupBy operation.\n\n\n\n\n\nName\nDescription\n\n\n\n\nagg\nCompute aggregations for each group of a group by operation.\n\n\nall\nAggregate the groups into Series.\n\n\ncount\nReturn the number of rows in each group.\n\n\nfirst\nAggregate the first values in the group.\n\n\nhead\nGet the first n rows of each group.\n\n\nlast\nAggregate the last values in the group.\n\n\nlen\nReturn the number of rows in each group.\n\n\nmap_groups\nApply a custom/user-defined function (UDF) over the groups as a sub-DataFrame.\n\n\nmax\nReduce the groups to the maximal value.\n\n\nmean\nReduce the groups to the mean values.\n\n\nmedian\nReturn the median per group.\n\n\nmin\nReduce the groups to the minimal value.\n\n\nn_unique\nCount the unique values per group.\n\n\nquantile\nCompute the quantile per group.\n\n\nsum\nReduce the groups to the sum.\n\n\ntail\nGet the last n rows of each group.\n\n\n\n\n\nGroupBy.agg(*aggs, **named_aggs)\nCompute aggregations for each group of a group by operation.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n*aggs\nIntoExpr | Iterable[IntoExpr]\nAggregations to compute for each group of the group by operation, specified as positional arguments. Accepts expression input. Strings are parsed as column names.\n()\n\n\n**named_aggs\nIntoExpr\nAdditional aggregations, specified as keyword arguments. The resulting columns will be renamed to the keyword used.\n{}\n\n\n\n\n\n\nCompute the aggregation of the columns for each group.\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [\"a\", \"b\", \"a\", \"b\", \"c\"],\n...         \"b\": [1, 2, 1, 3, 3],\n...         \"c\": [5, 4, 3, 2, 1],\n...     }\n... )\n&gt;&gt;&gt; df.group_by(\"a\").agg(pl.col(\"b\"), pl.col(\"c\"))\nshape: (3, 3)\n┌─────┬───────────┬───────────┐\n│ a   ┆ b         ┆ c         │\n│ --- ┆ ---       ┆ ---       │\n│ str ┆ list[i64] ┆ list[i64] │\n╞═════╪═══════════╪═══════════╡\n│ a   ┆ [1, 1]    ┆ [5, 3]    │\n├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤\n│ b   ┆ [2, 3]    ┆ [4, 2]    │\n├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤\n│ c   ┆ [3]       ┆ [1]       │\n└─────┴───────────┴───────────┘\nCompute the sum of a column for each group.\n&gt;&gt;&gt; df.group_by(\"a\").agg(pl.col(\"b\").sum())\nshape: (3, 2)\n┌─────┬─────┐\n│ a   ┆ b   │\n│ --- ┆ --- │\n│ str ┆ i64 │\n╞═════╪═════╡\n│ a   ┆ 2   │\n│ b   ┆ 5   │\n│ c   ┆ 3   │\n└─────┴─────┘\nCompute multiple aggregates at once by passing a list of expressions.\n&gt;&gt;&gt; df.group_by(\"a\").agg([pl.sum(\"b\"), pl.mean(\"c\")])\nshape: (3, 3)\n┌─────┬─────┬─────┐\n│ a   ┆ b   ┆ c   │\n│ --- ┆ --- ┆ --- │\n│ str ┆ i64 ┆ f64 │\n╞═════╪═════╪═════╡\n│ c   ┆ 3   ┆ 1.0 │\n│ a   ┆ 2   ┆ 4.0 │\n│ b   ┆ 5   ┆ 3.0 │\n└─────┴─────┴─────┘\nOr use positional arguments to compute multiple aggregations in the same way.\n&gt;&gt;&gt; df.group_by(\"a\").agg(\n...     pl.sum(\"b\").name.suffix(\"_sum\"),\n...     (pl.col(\"c\") ** 2).mean().name.suffix(\"_mean_squared\"),\n... )\nshape: (3, 3)\n┌─────┬───────┬────────────────┐\n│ a   ┆ b_sum ┆ c_mean_squared │\n│ --- ┆ ---   ┆ ---            │\n│ str ┆ i64   ┆ f64            │\n╞═════╪═══════╪════════════════╡\n│ a   ┆ 2     ┆ 17.0           │\n│ c   ┆ 3     ┆ 1.0            │\n│ b   ┆ 5     ┆ 10.0           │\n└─────┴───────┴────────────────┘\nUse keyword arguments to easily name your expression inputs.\n&gt;&gt;&gt; df.group_by(\"a\").agg(\n...     b_sum=pl.sum(\"b\"),\n...     c_mean_squared=(pl.col(\"c\") ** 2).mean(),\n... )\nshape: (3, 3)\n┌─────┬───────┬────────────────┐\n│ a   ┆ b_sum ┆ c_mean_squared │\n│ --- ┆ ---   ┆ ---            │\n│ str ┆ i64   ┆ f64            │\n╞═════╪═══════╪════════════════╡\n│ a   ┆ 2     ┆ 17.0           │\n│ c   ┆ 3     ┆ 1.0            │\n│ b   ┆ 5     ┆ 10.0           │\n└─────┴───────┴────────────────┘\n\n\n\n\nGroupBy.all()\nAggregate the groups into Series.\n\n\n&gt;&gt;&gt; df = pl.DataFrame({\"a\": [\"one\", \"two\", \"one\", \"two\"], \"b\": [1, 2, 3, 4]})\n&gt;&gt;&gt; df.group_by(\"a\", maintain_order=True).all()\nshape: (2, 2)\n┌─────┬───────────┐\n│ a   ┆ b         │\n│ --- ┆ ---       │\n│ str ┆ list[i64] │\n╞═════╪═══════════╡\n│ one ┆ [1, 3]    │\n│ two ┆ [2, 4]    │\n└─────┴───────────┘\n\n\n\n\nGroupBy.count()\nReturn the number of rows in each group.\n.. deprecated:: 0.20.5 This method has been renamed to :func:GroupBy.len.\nRows containing null values count towards the total.\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [\"Apple\", \"Apple\", \"Orange\"],\n...         \"b\": [1, None, 2],\n...     }\n... )\n&gt;&gt;&gt; df.group_by(\"a\").count()\nshape: (2, 2)\n┌────────┬───────┐\n│ a      ┆ count │\n│ ---    ┆ ---   │\n│ str    ┆ u32   │\n╞════════╪═══════╡\n│ Apple  ┆ 2     │\n│ Orange ┆ 1     │\n└────────┴───────┘\n\n\n\n\nGroupBy.first()\nAggregate the first values in the group.\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 2, 2, 3, 4, 5],\n...         \"b\": [0.5, 0.5, 4, 10, 13, 14],\n...         \"c\": [True, True, True, False, False, True],\n...         \"d\": [\"Apple\", \"Orange\", \"Apple\", \"Apple\", \"Banana\", \"Banana\"],\n...     }\n... )\n&gt;&gt;&gt; df.group_by(\"d\", maintain_order=True).first()\nshape: (3, 4)\n┌────────┬─────┬──────┬───────┐\n│ d      ┆ a   ┆ b    ┆ c     │\n│ ---    ┆ --- ┆ ---  ┆ ---   │\n│ str    ┆ i64 ┆ f64  ┆ bool  │\n╞════════╪═════╪══════╪═══════╡\n│ Apple  ┆ 1   ┆ 0.5  ┆ true  │\n│ Orange ┆ 2   ┆ 0.5  ┆ true  │\n│ Banana ┆ 4   ┆ 13.0 ┆ false │\n└────────┴─────┴──────┴───────┘\n\n\n\n\nGroupBy.head(n=5)\nGet the first n rows of each group.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nn\nint\nNumber of rows to return.\n5\n\n\n\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"letters\": [\"c\", \"c\", \"a\", \"c\", \"a\", \"b\"],\n...         \"nrs\": [1, 2, 3, 4, 5, 6],\n...     }\n... )\n&gt;&gt;&gt; df\nshape: (6, 2)\n┌─────────┬─────┐\n│ letters ┆ nrs │\n│ ---     ┆ --- │\n│ str     ┆ i64 │\n╞═════════╪═════╡\n│ c       ┆ 1   │\n│ c       ┆ 2   │\n│ a       ┆ 3   │\n│ c       ┆ 4   │\n│ a       ┆ 5   │\n│ b       ┆ 6   │\n└─────────┴─────┘\n&gt;&gt;&gt; df.group_by(\"letters\").head(2).sort(\"letters\")\nshape: (5, 2)\n┌─────────┬─────┐\n│ letters ┆ nrs │\n│ ---     ┆ --- │\n│ str     ┆ i64 │\n╞═════════╪═════╡\n│ a       ┆ 3   │\n│ a       ┆ 5   │\n│ b       ┆ 6   │\n│ c       ┆ 1   │\n│ c       ┆ 2   │\n└─────────┴─────┘\n\n\n\n\nGroupBy.last()\nAggregate the last values in the group.\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 2, 2, 3, 4, 5],\n...         \"b\": [0.5, 0.5, 4, 10, 14, 13],\n...         \"c\": [True, True, True, False, False, True],\n...         \"d\": [\"Apple\", \"Orange\", \"Apple\", \"Apple\", \"Banana\", \"Banana\"],\n...     }\n... )\n&gt;&gt;&gt; df.group_by(\"d\", maintain_order=True).last()\nshape: (3, 4)\n┌────────┬─────┬──────┬───────┐\n│ d      ┆ a   ┆ b    ┆ c     │\n│ ---    ┆ --- ┆ ---  ┆ ---   │\n│ str    ┆ i64 ┆ f64  ┆ bool  │\n╞════════╪═════╪══════╪═══════╡\n│ Apple  ┆ 3   ┆ 10.0 ┆ false │\n│ Orange ┆ 2   ┆ 0.5  ┆ true  │\n│ Banana ┆ 5   ┆ 13.0 ┆ true  │\n└────────┴─────┴──────┴───────┘\n\n\n\n\nGroupBy.len(name=None)\nReturn the number of rows in each group.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr | None\nAssign a name to the resulting column; if unset, defaults to “len”.\nNone\n\n\n\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame({\"a\": [\"Apple\", \"Apple\", \"Orange\"], \"b\": [1, None, 2]})\n&gt;&gt;&gt; df.group_by(\"a\").len()\nshape: (2, 2)\n┌────────┬─────┐\n│ a      ┆ len │\n│ ---    ┆ --- │\n│ str    ┆ u32 │\n╞════════╪═════╡\n│ Apple  ┆ 2   │\n│ Orange ┆ 1   │\n└────────┴─────┘\n&gt;&gt;&gt; df.group_by(\"a\").len(name=\"n\")\nshape: (2, 2)\n┌────────┬─────┐\n│ a      ┆ n   │\n│ ---    ┆ --- │\n│ str    ┆ u32 │\n╞════════╪═════╡\n│ Apple  ┆ 2   │\n│ Orange ┆ 1   │\n└────────┴─────┘\n\n\n\n\nGroupBy.map_groups(function)\nApply a custom/user-defined function (UDF) over the groups as a sub-DataFrame.\n.. warning:: This method is much slower than the native expressions API. Only use it if you cannot implement your logic otherwise.\nImplementing logic using a Python function is almost always significantly slower and more memory intensive than implementing the same logic using the native expression API because:\n\nThe native expression engine runs in Rust; UDFs run in Python.\nUse of Python UDFs forces the DataFrame to be materialized in memory.\nPolars-native expressions can be parallelised (UDFs cannot).\nPolars-native expressions can be logically optimised (UDFs cannot).\n\nWherever possible you should strongly prefer the native expression API to achieve the best performance.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunction\nCallable[[DataFrame], DataFrame]\nCustom function that receives a DataFrame and returns a DataFrame.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nDataFrame\n\n\n\n\n\n\n\nFor each color group sample two rows:\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"id\": [0, 1, 2, 3, 4],\n...         \"color\": [\"red\", \"green\", \"green\", \"red\", \"red\"],\n...         \"shape\": [\"square\", \"triangle\", \"square\", \"triangle\", \"square\"],\n...     }\n... )\n&gt;&gt;&gt; df.group_by(\"color\").map_groups(\n...     lambda group_df: group_df.sample(2)\n... )\nshape: (4, 3)\n┌─────┬───────┬──────────┐\n│ id  ┆ color ┆ shape    │\n│ --- ┆ ---   ┆ ---      │\n│ i64 ┆ str   ┆ str      │\n╞═════╪═══════╪══════════╡\n│ 1   ┆ green ┆ triangle │\n│ 2   ┆ green ┆ square   │\n│ 4   ┆ red   ┆ square   │\n│ 3   ┆ red   ┆ triangle │\n└─────┴───────┴──────────┘\nIt is better to implement this with an expression:\n&gt;&gt;&gt; df.filter(\n...     pl.int_range(pl.len()).shuffle().over(\"color\") &lt; 2\n... )\n\n\n\n\nGroupBy.max()\nReduce the groups to the maximal value.\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 2, 2, 3, 4, 5],\n...         \"b\": [0.5, 0.5, 4, 10, 13, 14],\n...         \"c\": [True, True, True, False, False, True],\n...         \"d\": [\"Apple\", \"Orange\", \"Apple\", \"Apple\", \"Banana\", \"Banana\"],\n...     }\n... )\n&gt;&gt;&gt; df.group_by(\"d\", maintain_order=True).max()\nshape: (3, 4)\n┌────────┬─────┬──────┬──────┐\n│ d      ┆ a   ┆ b    ┆ c    │\n│ ---    ┆ --- ┆ ---  ┆ ---  │\n│ str    ┆ i64 ┆ f64  ┆ bool │\n╞════════╪═════╪══════╪══════╡\n│ Apple  ┆ 3   ┆ 10.0 ┆ true │\n│ Orange ┆ 2   ┆ 0.5  ┆ true │\n│ Banana ┆ 5   ┆ 14.0 ┆ true │\n└────────┴─────┴──────┴──────┘\n\n\n\n\nGroupBy.mean()\nReduce the groups to the mean values.\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 2, 2, 3, 4, 5],\n...         \"b\": [0.5, 0.5, 4, 10, 13, 14],\n...         \"c\": [True, True, True, False, False, True],\n...         \"d\": [\"Apple\", \"Orange\", \"Apple\", \"Apple\", \"Banana\", \"Banana\"],\n...     }\n... )\n&gt;&gt;&gt; df.group_by(\"d\", maintain_order=True).mean()\nshape: (3, 4)\n┌────────┬─────┬──────────┬──────────┐\n│ d      ┆ a   ┆ b        ┆ c        │\n│ ---    ┆ --- ┆ ---      ┆ ---      │\n│ str    ┆ f64 ┆ f64      ┆ f64      │\n╞════════╪═════╪══════════╪══════════╡\n│ Apple  ┆ 2.0 ┆ 4.833333 ┆ 0.666667 │\n│ Orange ┆ 2.0 ┆ 0.5      ┆ 1.0      │\n│ Banana ┆ 4.5 ┆ 13.5     ┆ 0.5      │\n└────────┴─────┴──────────┴──────────┘\n\n\n\n\nGroupBy.median()\nReturn the median per group.\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 2, 2, 3, 4, 5],\n...         \"b\": [0.5, 0.5, 4, 10, 13, 14],\n...         \"d\": [\"Apple\", \"Banana\", \"Apple\", \"Apple\", \"Banana\", \"Banana\"],\n...     }\n... )\n&gt;&gt;&gt; df.group_by(\"d\", maintain_order=True).median()\nshape: (2, 3)\n┌────────┬─────┬──────┐\n│ d      ┆ a   ┆ b    │\n│ ---    ┆ --- ┆ ---  │\n│ str    ┆ f64 ┆ f64  │\n╞════════╪═════╪══════╡\n│ Apple  ┆ 2.0 ┆ 4.0  │\n│ Banana ┆ 4.0 ┆ 13.0 │\n└────────┴─────┴──────┘\n\n\n\n\nGroupBy.min()\nReduce the groups to the minimal value.\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 2, 2, 3, 4, 5],\n...         \"b\": [0.5, 0.5, 4, 10, 13, 14],\n...         \"c\": [True, True, True, False, False, True],\n...         \"d\": [\"Apple\", \"Orange\", \"Apple\", \"Apple\", \"Banana\", \"Banana\"],\n...     }\n... )\n&gt;&gt;&gt; df.group_by(\"d\", maintain_order=True).min()\nshape: (3, 4)\n┌────────┬─────┬──────┬───────┐\n│ d      ┆ a   ┆ b    ┆ c     │\n│ ---    ┆ --- ┆ ---  ┆ ---   │\n│ str    ┆ i64 ┆ f64  ┆ bool  │\n╞════════╪═════╪══════╪═══════╡\n│ Apple  ┆ 1   ┆ 0.5  ┆ false │\n│ Orange ┆ 2   ┆ 0.5  ┆ true  │\n│ Banana ┆ 4   ┆ 13.0 ┆ false │\n└────────┴─────┴──────┴───────┘\n\n\n\n\nGroupBy.n_unique()\nCount the unique values per group.\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 2, 1, 3, 4, 5],\n...         \"b\": [0.5, 0.5, 0.5, 10, 13, 14],\n...         \"d\": [\"Apple\", \"Banana\", \"Apple\", \"Apple\", \"Banana\", \"Banana\"],\n...     }\n... )\n&gt;&gt;&gt; df.group_by(\"d\", maintain_order=True).n_unique()\nshape: (2, 3)\n┌────────┬─────┬─────┐\n│ d      ┆ a   ┆ b   │\n│ ---    ┆ --- ┆ --- │\n│ str    ┆ u32 ┆ u32 │\n╞════════╪═════╪═════╡\n│ Apple  ┆ 2   ┆ 2   │\n│ Banana ┆ 3   ┆ 3   │\n└────────┴─────┴─────┘\n\n\n\n\nGroupBy.quantile(quantile, interpolation='nearest')\nCompute the quantile per group.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nquantile\nfloat\nQuantile between 0.0 and 1.0.\nrequired\n\n\ninterpolation\n(‘nearest’, ‘higher’, ‘lower’, ‘midpoint’, ‘linear’)\nInterpolation method.\n'nearest'\n\n\n\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 2, 2, 3, 4, 5],\n...         \"b\": [0.5, 0.5, 4, 10, 13, 14],\n...         \"d\": [\"Apple\", \"Orange\", \"Apple\", \"Apple\", \"Banana\", \"Banana\"],\n...     }\n... )\n&gt;&gt;&gt; df.group_by(\"d\", maintain_order=True).quantile(1)\nshape: (3, 3)\n┌────────┬─────┬──────┐\n│ d      ┆ a   ┆ b    │\n│ ---    ┆ --- ┆ ---  │\n│ str    ┆ f64 ┆ f64  │\n╞════════╪═════╪══════╡\n│ Apple  ┆ 3.0 ┆ 10.0 │\n│ Orange ┆ 2.0 ┆ 0.5  │\n│ Banana ┆ 5.0 ┆ 14.0 │\n└────────┴─────┴──────┘\n\n\n\n\nGroupBy.sum()\nReduce the groups to the sum.\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 2, 2, 3, 4, 5],\n...         \"b\": [0.5, 0.5, 4, 10, 13, 14],\n...         \"c\": [True, True, True, False, False, True],\n...         \"d\": [\"Apple\", \"Orange\", \"Apple\", \"Apple\", \"Banana\", \"Banana\"],\n...     }\n... )\n&gt;&gt;&gt; df.group_by(\"d\", maintain_order=True).sum()\nshape: (3, 4)\n┌────────┬─────┬──────┬─────┐\n│ d      ┆ a   ┆ b    ┆ c   │\n│ ---    ┆ --- ┆ ---  ┆ --- │\n│ str    ┆ i64 ┆ f64  ┆ u32 │\n╞════════╪═════╪══════╪═════╡\n│ Apple  ┆ 6   ┆ 14.5 ┆ 2   │\n│ Orange ┆ 2   ┆ 0.5  ┆ 1   │\n│ Banana ┆ 9   ┆ 27.0 ┆ 1   │\n└────────┴─────┴──────┴─────┘\n\n\n\n\nGroupBy.tail(n=5)\nGet the last n rows of each group.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nn\nint\nNumber of rows to return.\n5\n\n\n\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"letters\": [\"c\", \"c\", \"a\", \"c\", \"a\", \"b\"],\n...         \"nrs\": [1, 2, 3, 4, 5, 6],\n...     }\n... )\n&gt;&gt;&gt; df\nshape: (6, 2)\n┌─────────┬─────┐\n│ letters ┆ nrs │\n│ ---     ┆ --- │\n│ str     ┆ i64 │\n╞═════════╪═════╡\n│ c       ┆ 1   │\n│ c       ┆ 2   │\n│ a       ┆ 3   │\n│ c       ┆ 4   │\n│ a       ┆ 5   │\n│ b       ┆ 6   │\n└─────────┴─────┘\n&gt;&gt;&gt; df.group_by(\"letters\").tail(2).sort(\"letters\")\nshape: (5, 2)\n┌─────────┬─────┐\n│ letters ┆ nrs │\n│ ---     ┆ --- │\n│ str     ┆ i64 │\n╞═════════╪═════╡\n│ a       ┆ 3   │\n│ a       ┆ 5   │\n│ b       ┆ 6   │\n│ c       ┆ 2   │\n│ c       ┆ 4   │\n└─────────┴─────┘\n\n\n\n\n\n\nRollingGroupBy(self, df, index_column, *, period, offset, closed, group_by)\nA rolling grouper.\nThis has an .agg method which will allow you to run all polars expressions in a group by context.\n\n\n\n\n\nName\nDescription\n\n\n\n\nagg\nCompute aggregations for each group of a group by operation.\n\n\nmap_groups\nApply a custom/user-defined function (UDF) over the groups as a new DataFrame.\n\n\n\n\n\nRollingGroupBy.agg(*aggs, **named_aggs)\nCompute aggregations for each group of a group by operation.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n*aggs\nIntoExpr | Iterable[IntoExpr]\nAggregations to compute for each group of the group by operation, specified as positional arguments. Accepts expression input. Strings are parsed as column names.\n()\n\n\n**named_aggs\nIntoExpr\nAdditional aggregations, specified as keyword arguments. The resulting columns will be renamed to the keyword used.\n{}\n\n\n\n\n\n\n\nRollingGroupBy.map_groups(function, schema)\nApply a custom/user-defined function (UDF) over the groups as a new DataFrame.\nUsing this is considered an anti-pattern as it will be very slow because:\n\nit forces the engine to materialize the whole DataFrames for the groups.\nit is not parallelized.\nit blocks optimizations as the passed python function is opaque to the optimizer.\n\nThe idiomatic way to apply custom functions over multiple columns is using:\npl.struct([my_columns]).map_elements(lambda struct_series: ..)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunction\nCallable[[DataFrame], DataFrame]\nFunction to apply over each group of the LazyFrame; it receives a DataFrame and should return a DataFrame.\nrequired\n\n\nschema\nSchemaDict | None\nSchema of the output function. This has to be known statically. If the given schema is incorrect, this is a bug in the caller’s query and may lead to errors. If set to None, polars assumes the schema is unchanged.\nrequired",
    "crumbs": [
      "Reference",
      "Main Functions",
      "group_by"
    ]
  },
  {
    "objectID": "notebooks/group_by.html#classes",
    "href": "notebooks/group_by.html#classes",
    "title": "group_by",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nDynamicGroupBy\nA dynamic grouper.\n\n\nGroupBy\nStarts a new GroupBy operation.\n\n\nRollingGroupBy\nA rolling grouper.\n\n\n\n\n\nDynamicGroupBy(self, df, index_column, *, every, period, offset, include_boundaries, closed, label, group_by, start_by)\nA dynamic grouper.\nThis has an .agg method which allows you to run all polars expressions in a group by context.\n\n\n\n\n\nName\nDescription\n\n\n\n\nagg\nCompute aggregations for each group of a group by operation.\n\n\nmap_groups\nApply a custom/user-defined function (UDF) over the groups as a new DataFrame.\n\n\n\n\n\nDynamicGroupBy.agg(*aggs, **named_aggs)\nCompute aggregations for each group of a group by operation.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n*aggs\nIntoExpr | Iterable[IntoExpr]\nAggregations to compute for each group of the group by operation, specified as positional arguments. Accepts expression input. Strings are parsed as column names.\n()\n\n\n**named_aggs\nIntoExpr\nAdditional aggregations, specified as keyword arguments. The resulting columns will be renamed to the keyword used.\n{}\n\n\n\n\n\n\n\nDynamicGroupBy.map_groups(function, schema)\nApply a custom/user-defined function (UDF) over the groups as a new DataFrame.\nUsing this is considered an anti-pattern as it will be very slow because:\n\nit forces the engine to materialize the whole DataFrames for the groups.\nit is not parallelized.\nit blocks optimizations as the passed python function is opaque to the optimizer.\n\nThe idiomatic way to apply custom functions over multiple columns is using:\npl.struct([my_columns]).map_elements(lambda struct_series: ..)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunction\nCallable[[DataFrame], DataFrame]\nFunction to apply over each group of the LazyFrame; it receives a DataFrame and should return a DataFrame.\nrequired\n\n\nschema\nSchemaDict | None\nSchema of the output function. This has to be known statically. If the given schema is incorrect, this is a bug in the caller’s query and may lead to errors. If set to None, polars assumes the schema is unchanged.\nrequired\n\n\n\n\n\n\n\n\n\nGroupBy(self, df, *by, maintain_order, **named_by)\nStarts a new GroupBy operation.\n\n\n\n\n\nName\nDescription\n\n\n\n\nagg\nCompute aggregations for each group of a group by operation.\n\n\nall\nAggregate the groups into Series.\n\n\ncount\nReturn the number of rows in each group.\n\n\nfirst\nAggregate the first values in the group.\n\n\nhead\nGet the first n rows of each group.\n\n\nlast\nAggregate the last values in the group.\n\n\nlen\nReturn the number of rows in each group.\n\n\nmap_groups\nApply a custom/user-defined function (UDF) over the groups as a sub-DataFrame.\n\n\nmax\nReduce the groups to the maximal value.\n\n\nmean\nReduce the groups to the mean values.\n\n\nmedian\nReturn the median per group.\n\n\nmin\nReduce the groups to the minimal value.\n\n\nn_unique\nCount the unique values per group.\n\n\nquantile\nCompute the quantile per group.\n\n\nsum\nReduce the groups to the sum.\n\n\ntail\nGet the last n rows of each group.\n\n\n\n\n\nGroupBy.agg(*aggs, **named_aggs)\nCompute aggregations for each group of a group by operation.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n*aggs\nIntoExpr | Iterable[IntoExpr]\nAggregations to compute for each group of the group by operation, specified as positional arguments. Accepts expression input. Strings are parsed as column names.\n()\n\n\n**named_aggs\nIntoExpr\nAdditional aggregations, specified as keyword arguments. The resulting columns will be renamed to the keyword used.\n{}\n\n\n\n\n\n\nCompute the aggregation of the columns for each group.\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [\"a\", \"b\", \"a\", \"b\", \"c\"],\n...         \"b\": [1, 2, 1, 3, 3],\n...         \"c\": [5, 4, 3, 2, 1],\n...     }\n... )\n&gt;&gt;&gt; df.group_by(\"a\").agg(pl.col(\"b\"), pl.col(\"c\"))\nshape: (3, 3)\n┌─────┬───────────┬───────────┐\n│ a   ┆ b         ┆ c         │\n│ --- ┆ ---       ┆ ---       │\n│ str ┆ list[i64] ┆ list[i64] │\n╞═════╪═══════════╪═══════════╡\n│ a   ┆ [1, 1]    ┆ [5, 3]    │\n├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤\n│ b   ┆ [2, 3]    ┆ [4, 2]    │\n├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤\n│ c   ┆ [3]       ┆ [1]       │\n└─────┴───────────┴───────────┘\nCompute the sum of a column for each group.\n&gt;&gt;&gt; df.group_by(\"a\").agg(pl.col(\"b\").sum())\nshape: (3, 2)\n┌─────┬─────┐\n│ a   ┆ b   │\n│ --- ┆ --- │\n│ str ┆ i64 │\n╞═════╪═════╡\n│ a   ┆ 2   │\n│ b   ┆ 5   │\n│ c   ┆ 3   │\n└─────┴─────┘\nCompute multiple aggregates at once by passing a list of expressions.\n&gt;&gt;&gt; df.group_by(\"a\").agg([pl.sum(\"b\"), pl.mean(\"c\")])\nshape: (3, 3)\n┌─────┬─────┬─────┐\n│ a   ┆ b   ┆ c   │\n│ --- ┆ --- ┆ --- │\n│ str ┆ i64 ┆ f64 │\n╞═════╪═════╪═════╡\n│ c   ┆ 3   ┆ 1.0 │\n│ a   ┆ 2   ┆ 4.0 │\n│ b   ┆ 5   ┆ 3.0 │\n└─────┴─────┴─────┘\nOr use positional arguments to compute multiple aggregations in the same way.\n&gt;&gt;&gt; df.group_by(\"a\").agg(\n...     pl.sum(\"b\").name.suffix(\"_sum\"),\n...     (pl.col(\"c\") ** 2).mean().name.suffix(\"_mean_squared\"),\n... )\nshape: (3, 3)\n┌─────┬───────┬────────────────┐\n│ a   ┆ b_sum ┆ c_mean_squared │\n│ --- ┆ ---   ┆ ---            │\n│ str ┆ i64   ┆ f64            │\n╞═════╪═══════╪════════════════╡\n│ a   ┆ 2     ┆ 17.0           │\n│ c   ┆ 3     ┆ 1.0            │\n│ b   ┆ 5     ┆ 10.0           │\n└─────┴───────┴────────────────┘\nUse keyword arguments to easily name your expression inputs.\n&gt;&gt;&gt; df.group_by(\"a\").agg(\n...     b_sum=pl.sum(\"b\"),\n...     c_mean_squared=(pl.col(\"c\") ** 2).mean(),\n... )\nshape: (3, 3)\n┌─────┬───────┬────────────────┐\n│ a   ┆ b_sum ┆ c_mean_squared │\n│ --- ┆ ---   ┆ ---            │\n│ str ┆ i64   ┆ f64            │\n╞═════╪═══════╪════════════════╡\n│ a   ┆ 2     ┆ 17.0           │\n│ c   ┆ 3     ┆ 1.0            │\n│ b   ┆ 5     ┆ 10.0           │\n└─────┴───────┴────────────────┘\n\n\n\n\nGroupBy.all()\nAggregate the groups into Series.\n\n\n&gt;&gt;&gt; df = pl.DataFrame({\"a\": [\"one\", \"two\", \"one\", \"two\"], \"b\": [1, 2, 3, 4]})\n&gt;&gt;&gt; df.group_by(\"a\", maintain_order=True).all()\nshape: (2, 2)\n┌─────┬───────────┐\n│ a   ┆ b         │\n│ --- ┆ ---       │\n│ str ┆ list[i64] │\n╞═════╪═══════════╡\n│ one ┆ [1, 3]    │\n│ two ┆ [2, 4]    │\n└─────┴───────────┘\n\n\n\n\nGroupBy.count()\nReturn the number of rows in each group.\n.. deprecated:: 0.20.5 This method has been renamed to :func:GroupBy.len.\nRows containing null values count towards the total.\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [\"Apple\", \"Apple\", \"Orange\"],\n...         \"b\": [1, None, 2],\n...     }\n... )\n&gt;&gt;&gt; df.group_by(\"a\").count()\nshape: (2, 2)\n┌────────┬───────┐\n│ a      ┆ count │\n│ ---    ┆ ---   │\n│ str    ┆ u32   │\n╞════════╪═══════╡\n│ Apple  ┆ 2     │\n│ Orange ┆ 1     │\n└────────┴───────┘\n\n\n\n\nGroupBy.first()\nAggregate the first values in the group.\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 2, 2, 3, 4, 5],\n...         \"b\": [0.5, 0.5, 4, 10, 13, 14],\n...         \"c\": [True, True, True, False, False, True],\n...         \"d\": [\"Apple\", \"Orange\", \"Apple\", \"Apple\", \"Banana\", \"Banana\"],\n...     }\n... )\n&gt;&gt;&gt; df.group_by(\"d\", maintain_order=True).first()\nshape: (3, 4)\n┌────────┬─────┬──────┬───────┐\n│ d      ┆ a   ┆ b    ┆ c     │\n│ ---    ┆ --- ┆ ---  ┆ ---   │\n│ str    ┆ i64 ┆ f64  ┆ bool  │\n╞════════╪═════╪══════╪═══════╡\n│ Apple  ┆ 1   ┆ 0.5  ┆ true  │\n│ Orange ┆ 2   ┆ 0.5  ┆ true  │\n│ Banana ┆ 4   ┆ 13.0 ┆ false │\n└────────┴─────┴──────┴───────┘\n\n\n\n\nGroupBy.head(n=5)\nGet the first n rows of each group.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nn\nint\nNumber of rows to return.\n5\n\n\n\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"letters\": [\"c\", \"c\", \"a\", \"c\", \"a\", \"b\"],\n...         \"nrs\": [1, 2, 3, 4, 5, 6],\n...     }\n... )\n&gt;&gt;&gt; df\nshape: (6, 2)\n┌─────────┬─────┐\n│ letters ┆ nrs │\n│ ---     ┆ --- │\n│ str     ┆ i64 │\n╞═════════╪═════╡\n│ c       ┆ 1   │\n│ c       ┆ 2   │\n│ a       ┆ 3   │\n│ c       ┆ 4   │\n│ a       ┆ 5   │\n│ b       ┆ 6   │\n└─────────┴─────┘\n&gt;&gt;&gt; df.group_by(\"letters\").head(2).sort(\"letters\")\nshape: (5, 2)\n┌─────────┬─────┐\n│ letters ┆ nrs │\n│ ---     ┆ --- │\n│ str     ┆ i64 │\n╞═════════╪═════╡\n│ a       ┆ 3   │\n│ a       ┆ 5   │\n│ b       ┆ 6   │\n│ c       ┆ 1   │\n│ c       ┆ 2   │\n└─────────┴─────┘\n\n\n\n\nGroupBy.last()\nAggregate the last values in the group.\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 2, 2, 3, 4, 5],\n...         \"b\": [0.5, 0.5, 4, 10, 14, 13],\n...         \"c\": [True, True, True, False, False, True],\n...         \"d\": [\"Apple\", \"Orange\", \"Apple\", \"Apple\", \"Banana\", \"Banana\"],\n...     }\n... )\n&gt;&gt;&gt; df.group_by(\"d\", maintain_order=True).last()\nshape: (3, 4)\n┌────────┬─────┬──────┬───────┐\n│ d      ┆ a   ┆ b    ┆ c     │\n│ ---    ┆ --- ┆ ---  ┆ ---   │\n│ str    ┆ i64 ┆ f64  ┆ bool  │\n╞════════╪═════╪══════╪═══════╡\n│ Apple  ┆ 3   ┆ 10.0 ┆ false │\n│ Orange ┆ 2   ┆ 0.5  ┆ true  │\n│ Banana ┆ 5   ┆ 13.0 ┆ true  │\n└────────┴─────┴──────┴───────┘\n\n\n\n\nGroupBy.len(name=None)\nReturn the number of rows in each group.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr | None\nAssign a name to the resulting column; if unset, defaults to “len”.\nNone\n\n\n\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame({\"a\": [\"Apple\", \"Apple\", \"Orange\"], \"b\": [1, None, 2]})\n&gt;&gt;&gt; df.group_by(\"a\").len()\nshape: (2, 2)\n┌────────┬─────┐\n│ a      ┆ len │\n│ ---    ┆ --- │\n│ str    ┆ u32 │\n╞════════╪═════╡\n│ Apple  ┆ 2   │\n│ Orange ┆ 1   │\n└────────┴─────┘\n&gt;&gt;&gt; df.group_by(\"a\").len(name=\"n\")\nshape: (2, 2)\n┌────────┬─────┐\n│ a      ┆ n   │\n│ ---    ┆ --- │\n│ str    ┆ u32 │\n╞════════╪═════╡\n│ Apple  ┆ 2   │\n│ Orange ┆ 1   │\n└────────┴─────┘\n\n\n\n\nGroupBy.map_groups(function)\nApply a custom/user-defined function (UDF) over the groups as a sub-DataFrame.\n.. warning:: This method is much slower than the native expressions API. Only use it if you cannot implement your logic otherwise.\nImplementing logic using a Python function is almost always significantly slower and more memory intensive than implementing the same logic using the native expression API because:\n\nThe native expression engine runs in Rust; UDFs run in Python.\nUse of Python UDFs forces the DataFrame to be materialized in memory.\nPolars-native expressions can be parallelised (UDFs cannot).\nPolars-native expressions can be logically optimised (UDFs cannot).\n\nWherever possible you should strongly prefer the native expression API to achieve the best performance.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunction\nCallable[[DataFrame], DataFrame]\nCustom function that receives a DataFrame and returns a DataFrame.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nDataFrame\n\n\n\n\n\n\n\nFor each color group sample two rows:\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"id\": [0, 1, 2, 3, 4],\n...         \"color\": [\"red\", \"green\", \"green\", \"red\", \"red\"],\n...         \"shape\": [\"square\", \"triangle\", \"square\", \"triangle\", \"square\"],\n...     }\n... )\n&gt;&gt;&gt; df.group_by(\"color\").map_groups(\n...     lambda group_df: group_df.sample(2)\n... )\nshape: (4, 3)\n┌─────┬───────┬──────────┐\n│ id  ┆ color ┆ shape    │\n│ --- ┆ ---   ┆ ---      │\n│ i64 ┆ str   ┆ str      │\n╞═════╪═══════╪══════════╡\n│ 1   ┆ green ┆ triangle │\n│ 2   ┆ green ┆ square   │\n│ 4   ┆ red   ┆ square   │\n│ 3   ┆ red   ┆ triangle │\n└─────┴───────┴──────────┘\nIt is better to implement this with an expression:\n&gt;&gt;&gt; df.filter(\n...     pl.int_range(pl.len()).shuffle().over(\"color\") &lt; 2\n... )\n\n\n\n\nGroupBy.max()\nReduce the groups to the maximal value.\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 2, 2, 3, 4, 5],\n...         \"b\": [0.5, 0.5, 4, 10, 13, 14],\n...         \"c\": [True, True, True, False, False, True],\n...         \"d\": [\"Apple\", \"Orange\", \"Apple\", \"Apple\", \"Banana\", \"Banana\"],\n...     }\n... )\n&gt;&gt;&gt; df.group_by(\"d\", maintain_order=True).max()\nshape: (3, 4)\n┌────────┬─────┬──────┬──────┐\n│ d      ┆ a   ┆ b    ┆ c    │\n│ ---    ┆ --- ┆ ---  ┆ ---  │\n│ str    ┆ i64 ┆ f64  ┆ bool │\n╞════════╪═════╪══════╪══════╡\n│ Apple  ┆ 3   ┆ 10.0 ┆ true │\n│ Orange ┆ 2   ┆ 0.5  ┆ true │\n│ Banana ┆ 5   ┆ 14.0 ┆ true │\n└────────┴─────┴──────┴──────┘\n\n\n\n\nGroupBy.mean()\nReduce the groups to the mean values.\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 2, 2, 3, 4, 5],\n...         \"b\": [0.5, 0.5, 4, 10, 13, 14],\n...         \"c\": [True, True, True, False, False, True],\n...         \"d\": [\"Apple\", \"Orange\", \"Apple\", \"Apple\", \"Banana\", \"Banana\"],\n...     }\n... )\n&gt;&gt;&gt; df.group_by(\"d\", maintain_order=True).mean()\nshape: (3, 4)\n┌────────┬─────┬──────────┬──────────┐\n│ d      ┆ a   ┆ b        ┆ c        │\n│ ---    ┆ --- ┆ ---      ┆ ---      │\n│ str    ┆ f64 ┆ f64      ┆ f64      │\n╞════════╪═════╪══════════╪══════════╡\n│ Apple  ┆ 2.0 ┆ 4.833333 ┆ 0.666667 │\n│ Orange ┆ 2.0 ┆ 0.5      ┆ 1.0      │\n│ Banana ┆ 4.5 ┆ 13.5     ┆ 0.5      │\n└────────┴─────┴──────────┴──────────┘\n\n\n\n\nGroupBy.median()\nReturn the median per group.\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 2, 2, 3, 4, 5],\n...         \"b\": [0.5, 0.5, 4, 10, 13, 14],\n...         \"d\": [\"Apple\", \"Banana\", \"Apple\", \"Apple\", \"Banana\", \"Banana\"],\n...     }\n... )\n&gt;&gt;&gt; df.group_by(\"d\", maintain_order=True).median()\nshape: (2, 3)\n┌────────┬─────┬──────┐\n│ d      ┆ a   ┆ b    │\n│ ---    ┆ --- ┆ ---  │\n│ str    ┆ f64 ┆ f64  │\n╞════════╪═════╪══════╡\n│ Apple  ┆ 2.0 ┆ 4.0  │\n│ Banana ┆ 4.0 ┆ 13.0 │\n└────────┴─────┴──────┘\n\n\n\n\nGroupBy.min()\nReduce the groups to the minimal value.\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 2, 2, 3, 4, 5],\n...         \"b\": [0.5, 0.5, 4, 10, 13, 14],\n...         \"c\": [True, True, True, False, False, True],\n...         \"d\": [\"Apple\", \"Orange\", \"Apple\", \"Apple\", \"Banana\", \"Banana\"],\n...     }\n... )\n&gt;&gt;&gt; df.group_by(\"d\", maintain_order=True).min()\nshape: (3, 4)\n┌────────┬─────┬──────┬───────┐\n│ d      ┆ a   ┆ b    ┆ c     │\n│ ---    ┆ --- ┆ ---  ┆ ---   │\n│ str    ┆ i64 ┆ f64  ┆ bool  │\n╞════════╪═════╪══════╪═══════╡\n│ Apple  ┆ 1   ┆ 0.5  ┆ false │\n│ Orange ┆ 2   ┆ 0.5  ┆ true  │\n│ Banana ┆ 4   ┆ 13.0 ┆ false │\n└────────┴─────┴──────┴───────┘\n\n\n\n\nGroupBy.n_unique()\nCount the unique values per group.\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 2, 1, 3, 4, 5],\n...         \"b\": [0.5, 0.5, 0.5, 10, 13, 14],\n...         \"d\": [\"Apple\", \"Banana\", \"Apple\", \"Apple\", \"Banana\", \"Banana\"],\n...     }\n... )\n&gt;&gt;&gt; df.group_by(\"d\", maintain_order=True).n_unique()\nshape: (2, 3)\n┌────────┬─────┬─────┐\n│ d      ┆ a   ┆ b   │\n│ ---    ┆ --- ┆ --- │\n│ str    ┆ u32 ┆ u32 │\n╞════════╪═════╪═════╡\n│ Apple  ┆ 2   ┆ 2   │\n│ Banana ┆ 3   ┆ 3   │\n└────────┴─────┴─────┘\n\n\n\n\nGroupBy.quantile(quantile, interpolation='nearest')\nCompute the quantile per group.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nquantile\nfloat\nQuantile between 0.0 and 1.0.\nrequired\n\n\ninterpolation\n(‘nearest’, ‘higher’, ‘lower’, ‘midpoint’, ‘linear’)\nInterpolation method.\n'nearest'\n\n\n\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 2, 2, 3, 4, 5],\n...         \"b\": [0.5, 0.5, 4, 10, 13, 14],\n...         \"d\": [\"Apple\", \"Orange\", \"Apple\", \"Apple\", \"Banana\", \"Banana\"],\n...     }\n... )\n&gt;&gt;&gt; df.group_by(\"d\", maintain_order=True).quantile(1)\nshape: (3, 3)\n┌────────┬─────┬──────┐\n│ d      ┆ a   ┆ b    │\n│ ---    ┆ --- ┆ ---  │\n│ str    ┆ f64 ┆ f64  │\n╞════════╪═════╪══════╡\n│ Apple  ┆ 3.0 ┆ 10.0 │\n│ Orange ┆ 2.0 ┆ 0.5  │\n│ Banana ┆ 5.0 ┆ 14.0 │\n└────────┴─────┴──────┘\n\n\n\n\nGroupBy.sum()\nReduce the groups to the sum.\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 2, 2, 3, 4, 5],\n...         \"b\": [0.5, 0.5, 4, 10, 13, 14],\n...         \"c\": [True, True, True, False, False, True],\n...         \"d\": [\"Apple\", \"Orange\", \"Apple\", \"Apple\", \"Banana\", \"Banana\"],\n...     }\n... )\n&gt;&gt;&gt; df.group_by(\"d\", maintain_order=True).sum()\nshape: (3, 4)\n┌────────┬─────┬──────┬─────┐\n│ d      ┆ a   ┆ b    ┆ c   │\n│ ---    ┆ --- ┆ ---  ┆ --- │\n│ str    ┆ i64 ┆ f64  ┆ u32 │\n╞════════╪═════╪══════╪═════╡\n│ Apple  ┆ 6   ┆ 14.5 ┆ 2   │\n│ Orange ┆ 2   ┆ 0.5  ┆ 1   │\n│ Banana ┆ 9   ┆ 27.0 ┆ 1   │\n└────────┴─────┴──────┴─────┘\n\n\n\n\nGroupBy.tail(n=5)\nGet the last n rows of each group.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nn\nint\nNumber of rows to return.\n5\n\n\n\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"letters\": [\"c\", \"c\", \"a\", \"c\", \"a\", \"b\"],\n...         \"nrs\": [1, 2, 3, 4, 5, 6],\n...     }\n... )\n&gt;&gt;&gt; df\nshape: (6, 2)\n┌─────────┬─────┐\n│ letters ┆ nrs │\n│ ---     ┆ --- │\n│ str     ┆ i64 │\n╞═════════╪═════╡\n│ c       ┆ 1   │\n│ c       ┆ 2   │\n│ a       ┆ 3   │\n│ c       ┆ 4   │\n│ a       ┆ 5   │\n│ b       ┆ 6   │\n└─────────┴─────┘\n&gt;&gt;&gt; df.group_by(\"letters\").tail(2).sort(\"letters\")\nshape: (5, 2)\n┌─────────┬─────┐\n│ letters ┆ nrs │\n│ ---     ┆ --- │\n│ str     ┆ i64 │\n╞═════════╪═════╡\n│ a       ┆ 3   │\n│ a       ┆ 5   │\n│ b       ┆ 6   │\n│ c       ┆ 2   │\n│ c       ┆ 4   │\n└─────────┴─────┘\n\n\n\n\n\n\nRollingGroupBy(self, df, index_column, *, period, offset, closed, group_by)\nA rolling grouper.\nThis has an .agg method which will allow you to run all polars expressions in a group by context.\n\n\n\n\n\nName\nDescription\n\n\n\n\nagg\nCompute aggregations for each group of a group by operation.\n\n\nmap_groups\nApply a custom/user-defined function (UDF) over the groups as a new DataFrame.\n\n\n\n\n\nRollingGroupBy.agg(*aggs, **named_aggs)\nCompute aggregations for each group of a group by operation.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n*aggs\nIntoExpr | Iterable[IntoExpr]\nAggregations to compute for each group of the group by operation, specified as positional arguments. Accepts expression input. Strings are parsed as column names.\n()\n\n\n**named_aggs\nIntoExpr\nAdditional aggregations, specified as keyword arguments. The resulting columns will be renamed to the keyword used.\n{}\n\n\n\n\n\n\n\nRollingGroupBy.map_groups(function, schema)\nApply a custom/user-defined function (UDF) over the groups as a new DataFrame.\nUsing this is considered an anti-pattern as it will be very slow because:\n\nit forces the engine to materialize the whole DataFrames for the groups.\nit is not parallelized.\nit blocks optimizations as the passed python function is opaque to the optimizer.\n\nThe idiomatic way to apply custom functions over multiple columns is using:\npl.struct([my_columns]).map_elements(lambda struct_series: ..)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunction\nCallable[[DataFrame], DataFrame]\nFunction to apply over each group of the LazyFrame; it receives a DataFrame and should return a DataFrame.\nrequired\n\n\nschema\nSchemaDict | None\nSchema of the output function. This has to be known statically. If the given schema is incorrect, this is a bug in the caller’s query and may lead to errors. If set to None, polars assumes the schema is unchanged.\nrequired",
    "crumbs": [
      "Reference",
      "Main Functions",
      "group_by"
    ]
  },
  {
    "objectID": "guides/onboarding.html",
    "href": "guides/onboarding.html",
    "title": "Contribution/Collaboration Guide",
    "section": "",
    "text": "Summary\n\nThe Northwest Pathogen Genomics Center of Excellence (NW-PaGe) uses a public Github organization to host our code.\nIf you want to contribute to the organization, please read this guide and our security guidelines.\nYou will need Git and Github to make code contributions:"
  },
  {
    "objectID": "guides/onboarding.html#before-you-make-code-changes",
    "href": "guides/onboarding.html#before-you-make-code-changes",
    "title": "Contribution/Collaboration Guide",
    "section": "Before you make code changes",
    "text": "Before you make code changes\nAfter creating a repo (Section 4) and/or cloning the repo (Section 5) into a your local machine, you can start writing and contributing code to the remote code base in Github.\n\nCheck that you have your local clone linked to the remote repo by running git status. It should tell you that you’re on the main branch\n\n\n\n\n\nterminal\n\ngit status\n\n\n\n\nWe need to refresh the repo and check for any code changes to make sure our local clone is up to date. Use git fetch to find changes and git pull to pull those changes into your local clone.\n\n\n\n\n\nterminal\n\ngit fetch\ngit pull\n\n\n\n\nMake a new branch so we can isolate your changes and prevent accidentally pushing code changes to the main branch\n\n\n\n\n\nterminal\n\ngit branch &lt;insert your branch name here&gt;\n\n\n\n\nSwitch to that branch\n\n\n\n\n\nterminal\n\ngit switch &lt;your branch name here&gt;\n\n\n\nNow you are working in the local branch that you created and you can begin writing code or making updates."
  },
  {
    "objectID": "guides/onboarding.html#committing-your-changes",
    "href": "guides/onboarding.html#committing-your-changes",
    "title": "Contribution/Collaboration Guide",
    "section": "Committing your changes",
    "text": "Committing your changes\nOnce you make changes you can commit them to the local branch you created. This is like saving your work to the branch. The branch can be pushed to the remote repo in Github, so you can continually make changes and push them to the remote where they will be stored safely.\nTo make a commit, save your work and then in the terminal write git commit -m \"&lt;your message here\", like this:\n\n\n\n\nterminal\n\ngit commit -m \"feat: this is a new feature!\"\n\n\n\nNote that the -m is a parameter to let you write a commit message. Commit messages are important so that other collaborators can understand what changes you made. You can write a description like this\n\n\n\n\nterminal\n\ngit commit -m \"feat: this is a new feature!\" \"this is a description. I made this feature in the code\"\n\n\n\nAlso note that I am using the word feat in the commit message. This is important word that can trigger a github action. We’ll cover it below in Section 6.5"
  },
  {
    "objectID": "guides/onboarding.html#make-a-pull-request",
    "href": "guides/onboarding.html#make-a-pull-request",
    "title": "Contribution/Collaboration Guide",
    "section": "Make a pull request",
    "text": "Make a pull request\nNow you have committed your changes, but your code has only been committed to a branch. In order to have your changes implemented in the main codebase you need to merge your branch into the main branch.\nWhen working in a collaborative team setting it is important to have your team review the changes you made before implementing them into the main branch. Everyone makes mistakes, and this is an opportunity to vet your code and have everyone sign off on the changes you want.\nAfter you make your commits, go back to Github in your browser and go to your repo. There should be a box that appears showing your commit and a button that says Compare & pull request\n\nClick that button and it will bring you to the Open a pull request page.\n\nSelect who you want to review your code and assign yourself.\nUse labels to tag what this pull request refers to (very helpful in search for changes when managing the project) and\nAdd a milestone if it applies.\n\nNote that labels, milestones, and projects are a way to keep track of changes and issues in your project. I highly recommend setting them up, more below.\nThis should automatically send an email to the reviewers that there is code needed to be merged to a branch."
  },
  {
    "objectID": "guides/onboarding.html#sec-merge",
    "href": "guides/onboarding.html#sec-merge",
    "title": "Contribution/Collaboration Guide",
    "section": "Merging a branch to main",
    "text": "Merging a branch to main\nTypically your teammates and the repo admin will review your code and merge your branch into the main branch.\nIn Github, click on the Pull requests tab. Here you will see open pull requests and you can click on the one you want to merge.\n\n\n\nReview a pull request\n\n\n\nIn the pull request you will see 4 tabs;\n\nConversation tab that shows all the comments, descriptions, tags, and more.\nCommits tab that contains a list of all the commits made in this request\nChecks - if you have automated testing or apps in the repo you can trigger them with a pull request and see them here. For example, you can set up automated unit tests to run whenever a pull request is made. A github action will run the unit test and pass (or fail) here before it is merged to main.\nFiles changed - I personally always flip through this tab because it shows all the differences (diffs) between the old codebase and the new commits that were made.\n\n\n\n\nFiles Changed\n\n\n\nWhen you are comfortable with merging these changes, you can either leave a comment, approve, or request further changes by clicking on the Review changes dropdown menu.\n\n\n\nReview changes\n\n\n\nYou can leave inline comments in the commits by viewing the file of choice, and then hovering over the line of interest and clicking the + sign:\n\n\n\nview file\n\n\n\n\n\ninline comment\n\n\nThese comments will be tagged in the pull request and will need to be resolved by the person making the request before the code can get merged into the main branch.\n\nOnce the pull request has the approvals needed, you can merge it. Note, admins can customize how approvals work. We normally just have one admin or person required to approve a pull request for it to be mergeable. To state again, in the Review changes dropdown there is an option to approve the request - that is what is required.\n\n\n\nmerge\n\n\nAfter approval, hopefully you have a message that says This branch has no conflicts with the base branch. If there are conflicts it will prevent you from merging. We require the user who made the commit to resolve merge conflicts. The conflict typically happens when your branch is out of date and it is not lined up with the current main branch. You sometimes need to merge the main branch into your local branch and then commit those changes. This is very scenario dependent and requires some googling. Please reach out to us for help if this happens!"
  },
  {
    "objectID": "guides/onboarding.html#sec-changelog",
    "href": "guides/onboarding.html#sec-changelog",
    "title": "Contribution/Collaboration Guide",
    "section": "Release Cycles and Changelogs",
    "text": "Release Cycles and Changelogs\nAs I mentioned before, I used special trigger words like feat or fix in commit messages. These words indicate that a commit contains a new code feature or a bug fix. They trigger a github action that will produce a changelog, documentation, and a version change in the code based when the branch gets merged to main. Please read more about this topic in release cycles. In summary, there are key words you can add to your commit message that will trigger certain actions.\nFor example, the word bug will trigger the version patch number to increase, meaning if your current codebase is on version v1.0.0 it will increase the version to v1.0.1 . Here’s what it looks like in Github:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe word feat will trigger a change in the minor version, so it will bump v1.0.0 to v1.1.0. There are many other words that can trigger actions and you can customize them to do what you want.\nI strongly recommend implementing this in your repo and working in a release cycle. To give an example, our team has a 1 month release cycle:\n\nweek 1: planning out code changes and fixes\nweek 2: writing out code and making pull requests\nweek 3: reviewing pull requests and testing them\nweek 4: preparing communications about the new changes and merging the code to main\n\nOnce the code is merged to main, we have a github action that automatically creates our versioning, change logs, documentation, and saves a snapshot of our codebase. The action looks for trigger words (like fix, feat or BREAKING CHANGE) and it will divide the commits that were merged into the main branch and write out all the documentation for the change log. It looks like this:\n\n\nNotice that I can:\n\nUse the version numbers as a tag and switch versions of my entire code base. Very useful if something breaks in production and you want to revert to the old code base, and it gives someone the option of downloading a specific version of your package\nI have a summary of all the commits (with links) that were included in that version\nI don’t need to manually do anything\nI can send that change log to leadership and show a high level view of the changes made, with the option to see granular details if wanted\nIt documents all of you conversations and changes (+1 for transparency! ➕)\n\nFor more details, follow the release cycles guide"
  },
  {
    "objectID": "guides/onboarding.html#github-project-management",
    "href": "guides/onboarding.html#github-project-management",
    "title": "Contribution/Collaboration Guide",
    "section": "Github project management",
    "text": "Github project management\nMilestones, projects, labels, etc."
  },
  {
    "objectID": "guides/guides.html",
    "href": "guides/guides.html",
    "title": "Repo Documentation",
    "section": "",
    "text": "This site was created using Quarto, Github, and uses a Github Action to automatically render when a commit is pushed to the main branch of this repository.\nQuarto is a framework for creating documentation, slideshows, articles, blogs, books and websites using markdown. It can execute R, Python and other programming languages within the document.\nGithub Actions uses a .yml file in the repository to trigger an action based on a certain event. In this case, when a commit is pushed to the main branch the .yml will trigger this Quarto website to render to the gh-pages branch of the repository and publish the github page. This section will give details on how to\n\nCreate the website\nCreate, edit, and troubleshoot the Github Action to render the site"
  },
  {
    "objectID": "guides/guides.html#editadd-sections-and-chapters",
    "href": "guides/guides.html#editadd-sections-and-chapters",
    "title": "Repo Documentation",
    "section": "Edit/Add Sections and Chapters",
    "text": "Edit/Add Sections and Chapters\nTo add a section, open up the _quarto.yml file and scroll to the navbar section\n\n\n\n_quarto.yml\n\nproject:\n  type: website\nwebsite:\n  title: \"COE Github Standards\"\n  search: true\n  \n  navbar: \n    background: primary\n    left: \n      - text: Home\n        href: index.qmd\n      - text: Github Organization Standards\n        menu: \n          - href: std/security.qmd\n            text: \"0: Security Standards\"\n          - href: std/lic.qmd\n            text: \"1: Choosing a License\"\n          - href: std/templates.qmd\n            text: \"2: Org Policy Setting\"\n\n\n\nThis is where all of the qmd files are sourced and the instructions on how to format and style the navigation bar in the website.\nCurrently, the project is set up to have each section have it’s own drop down menu in the navbar. In a section, use - href: to specify a file and text: to give the file a custom name in the website.\nEach chapter exists within a sub-folder, so to add a chapter make sure create the qmd in its sub-folder and then reference the sub-folder and chapter in the .yml. For example, if you make a new chapter called new-chapter.qmd and it exists in the covid section/sub-folder, you need to reference it in the .yml file like: covid/new-chapter.qmd"
  },
  {
    "objectID": "guides/guides.html#website-style",
    "href": "guides/guides.html#website-style",
    "title": "Repo Documentation",
    "section": "Website Style",
    "text": "Website Style\nYou can customize many aspects of the website in the .yml file itself with the format: function. There are a ton of themes included in Quarto here and you can also add a custom css and/or scss file to your project. I think you can even go super in depth and customize the javascript components of the site, but I’m not entirely sure how to do that yet. This website has a ton of custom css components with Quarto, and possibly uses custom javascript, so it could be a place to start if you’re interested. Basically, you need to embed the css file into your _quarto.yml file\n\n\n\n_quarto.yml\n\nformat:\n  html:\n    theme: \n      - cosmo\n      - assets/styles.scss\n    scss: assets/styles.scss\n    # css: styles.css\n    toc: true\n    highlight-style: assets/custom.theme"
  },
  {
    "objectID": "guides/guides.html#open-the-r-project",
    "href": "guides/guides.html#open-the-r-project",
    "title": "Repo Documentation",
    "section": "Open the R project",
    "text": "Open the R project\nThis is a Quarto website that is contained in a .rproj file path. The R project contains all the documents used to create this website. Begin by opening the R project when should be in your local clone under C:\\Users\\XXXXXXX\\Projects\\Sequencing_2.0\\sequencing_documentation\\sequencing_documentation.Rproj"
  },
  {
    "objectID": "guides/guides.html#open-the-files",
    "href": "guides/guides.html#open-the-files",
    "title": "Repo Documentation",
    "section": "Open the files",
    "text": "Open the files\nThis project has .qmd files (Quarto Markdown files) that each represent a chapter in the website. All of the .qmd files are knitted together (using R knitr) which compiles all of the files to be sourced into htmls.\nThis website is set up to have each major section contain multiple chapters. To open a chapter, the bottom right pane in your R Studio window should contain folders for each section, highlighted below\n\n$ tree /f\nC:.\n│   .gitignore\n│   about.qmd\n│   index.qmd\n│   standards.Rproj\n│   _quarto.yml\n│\n├───assets\n│       custom.theme\n│       styles.css\n│       styles.scss\n│\n├───std\n│   │   creds.qmd\n│   │   lic.qmd\n│   │   public_code.qmd\n│   │   security.qmd\n│   │   templates.qmd\n│   │\n│   └───images\n│\n├───tools\n│   │   how_to.qmd\n│   │   iac.qmd\n│   │   link_code.qmd\n│   │   readme.qmd\n│   │   release.qmd\n│   │   renv.qmd\n│   │   teams.qmd\n│   │\n│   └───images\n\n\nThe .qmd files are inside of these folders. Select one to edit."
  },
  {
    "objectID": "guides/guides.html#commit-changes",
    "href": "guides/guides.html#commit-changes",
    "title": "Repo Documentation",
    "section": "Commit changes",
    "text": "Commit changes\nOnce you’re done editing, push the change to the main branch (or make a new branch, and then a pull request for the main branch). More one this in the git chapter"
  },
  {
    "objectID": "guides/guides.html#example-yaml-workflow",
    "href": "guides/guides.html#example-yaml-workflow",
    "title": "Repo Documentation",
    "section": "Example YAML Workflow",
    "text": "Example YAML Workflow\nThe .yml workflow for this project looks something like this:\non: is a tag indicating when the action will run. Right now it will run when any code gets pushed to the main branch in the documentation folder or lineages_public_repo.R script\n\n\n\nquarto-publish.yml\n\non: \n  push:\n    branches:\n      - main\n    paths:\n      - documentation/**\n      - lineages_public_repo.R\n\n\njobs: is a tag that tells a Github virtual machine what to run and what operating system to run it on. In this case ubuntu with the latest version. This can be windows, linux or macOS.\n\n\n\nquarto-publish.yml\n\nname: Render and Publish\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n\n\n\nNow we have the steps:\n\nenv will find the renv folder\nuses: actions/checkout@v3 will refresh the repo and pull the latest changes\nuses: quarto-dev/quarto-actions/setup@v2 will install quarto\nuses: actions/cache@v1 and the code below it will set up renv and use the cached packages to install them onto the Github virtual machine\n\n\n\n\nquarto-publish.yml\n\n    env:\n      RENV_PATHS_ROOT: ~/.cache/R/renv\n      \n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v3\n        \n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n        \n      - name: Prep CURL install\n        run: sudo apt-get update\n\n      - name: Install CURL Headers\n        run: sudo apt-get install libcurl4-openssl-dev\n\n      # - name: Setup Renv\n      #   uses: r-lib/actions/setup-renv@v2\n  \n      - name: Cache packages\n        uses: actions/cache@v1\n        with:\n          path: ${{ env.RENV_PATHS_ROOT }}\n          key: ${{ runner.os }}-renv-${{ hashFiles('**/renv.lock') }}\n          restore-keys: |\n            ${{ runner.os }}-renv-\n      \n      - name: Restore packages\n        shell: Rscript {0}\n        run: |\n          if (!requireNamespace(\"renv\", quietly = TRUE)) install.packages(\"renv\")\n          renv::restore()\n\n\n\nAnd finally,\n\nuses: quarto-dev/quarto-actions/publish@v2 will render the site by running quarto render\nwith: target: gh-pages path: documentation/_site lets you know which branch and path to render the site to\n\n\n\n\nquarto-publish.yml\n\n      - name: Publish to GitHub Pages (and render)\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: gh-pages\n          path: documentation/_site\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} # this secret is always available for github actions"
  },
  {
    "objectID": "guides/guides.html#using-renv-in-the-gh-action",
    "href": "guides/guides.html#using-renv-in-the-gh-action",
    "title": "Repo Documentation",
    "section": "Using renv in the GH Action",
    "text": "Using renv in the GH Action\nIf you need to constantly update your website with code chunk, this is the best way to do it. It is also probably safer and better than the _freeze way, but it requires a better understanding of Github Actions and virtual environments.\nrenv is an R package for creating a project level virtual environment. In other words, renv will create project specific folders that contain the specific R package versions you use in an project. More on virtual environments here\nTo use renv in a Github Action, you can put\n\n\n\n\nquarto-publish.yml\n\n- name: Setup Renv\n  uses: r-lib/actions/setup-renv@v2\n\n\n\nor use the renv cache code in the yaml section above"
  },
  {
    "objectID": "guides/guides.html#using-a-_freeze-file",
    "href": "guides/guides.html#using-a-_freeze-file",
    "title": "Repo Documentation",
    "section": "Using a _freeze file",
    "text": "Using a _freeze file\nIf you only need to execute the code once or just need to render a non-executable code chunk once, make sure you have this code in your _quarto.yml file:\n\n\n\n\n_quarto.yml\n\nexecute:\n  error: true\n  freeze: true\n\n\n\nand then run this in your terminal window:\nquarto render name-of-specific-document-or-chapter.qmd\nThis will render that specific document in the website, execute code chunks if they are set to execute (eval: true) and then it will create a _freeze file. The _freeze file will save a snapshot of that specific document and not re-render it in the Github Action. This means you can render other parts of the website, but any files in the _freeze folder will stay the same as they are in the freeze. If you need to make changes to a freeze document, run the quarto render code again after making changes.\nThis is also documented in the Quarto dev documentation"
  },
  {
    "objectID": "guides/guides.html#troubleshooting",
    "href": "guides/guides.html#troubleshooting",
    "title": "Repo Documentation",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nSo you did these steps:\n\nCreate the quarto-publish.yml\nRun quarto publish gh-pages in the terminal\nPush all the files in your git to the main branch\n\nIf this works on your first try then the universe is taking extra special care of you.\nIf not, you are like the rest of us poor souls:/\n\nThe first thing I would check is the error in your Github repo’s Action tab.\nIf the error is something like jsonlite not installed or some package not installed then it most likely means your are trying to commit a chunk of code in the documentation. Even if you are not executing the code, Github Actions will punish you. There are a couple options to fix this, depending on your priorities.\n\nIf you don’t care about executing your code and/or only need to push that part of the script once, consider using the _freeze option\nIf you need to execute code or need to programmtically render the document with code chunks often, consider using renv or a similar package installation method"
  },
  {
    "objectID": "articles/index.html",
    "href": "articles/index.html",
    "title": "Licensing",
    "section": "",
    "text": "Summary\n\nLicenses prevent code theft and inappropriate redistribution of code.\nReview common open-source licenses\nLicense types vary depending on repo goals\n\n\n\n\nGeneral License Info\nBelow is a list of common open-source licenses.\n\n\nThere isn’t a one size fits all license, so thankfully there are a variety of options. Here are two common ones:\n\n\nGNU GPL licenses\n\nThese are the strong licenses\nPrevents someone from taking our code and privatizing it (and making money off of it)\nSomeone can still use our code, they just need to ensure that what they’re doing with it is open-source\n“Copyright and license notices must be preserved.”\n“Contributors provide an express grant of patent rights. When a modified version is used to provide a service over a network, the complete source code of the modified version must be made available.”\n\n\n\nMIT license\n\nI think this is the most commonly used one\n“short and simple permissive license… only requiring preservation of copyright and license notices”\n“Licensed works, modifications, and larger works may be distributed under different terms and without source code.”\nSomeone could basically do whatever they want with the code.\nNextstain/ncov repo is currently using this\n\nAnd here are a couple of youtube videos that were helping in explaining licensing"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\n\nNORTHWEST PATHOGEN GENOMICS CENTER OF EXCELLENCE\n\ndiqa databricks sequencing pipeline\n",
    "section": "",
    "text": "NORTHWEST PATHOGEN GENOMICS CENTER OF EXCELLENCE\n\ndiqa databricks sequencing pipeline\n\nThis site documents the Data Integration and Quality Assurance team’s (DIQA) sequencing metadata pipeline. Here you can find onboarding and installation guides and documentation on our functions and scripts.\nGet started\n\n\n\n\n\n\n  \n\n\n\nUser Guides\n\n\n\n\n\n\n\nCode Reference\n\n\n\n\n\n\n\n\nArticles"
  },
  {
    "objectID": "guides/index.html",
    "href": "guides/index.html",
    "title": "Guides and Onboarding",
    "section": "",
    "text": "Onboarding\n\nInstallation\nGet project access\nGetting started with desk manuals\nFlowcharts and pipeline documentation\n\n\n\n\n\n\nGuides\n\nGithub guides\nDatabricks/Azure guides\nTutorials and tech troubleshooting"
  },
  {
    "objectID": "notebooks/index.html",
    "href": "notebooks/index.html",
    "title": "Function reference",
    "section": "",
    "text": "Functions to inspect docstrings.\n\n\n\nfunctions\n\n\n\ngroup_by\n\n\n\n\n\n\n\nScripts and notebooks for the pipeline\n\n\n\nscript\nBetter CSV Storage",
    "crumbs": [
      "Reference",
      "Function reference"
    ]
  },
  {
    "objectID": "notebooks/index.html#main-functions",
    "href": "notebooks/index.html#main-functions",
    "title": "Function reference",
    "section": "",
    "text": "Functions to inspect docstrings.\n\n\n\nfunctions\n\n\n\ngroup_by",
    "crumbs": [
      "Reference",
      "Function reference"
    ]
  },
  {
    "objectID": "notebooks/index.html#notebooks",
    "href": "notebooks/index.html#notebooks",
    "title": "Function reference",
    "section": "",
    "text": "Scripts and notebooks for the pipeline\n\n\n\nscript\nBetter CSV Storage",
    "crumbs": [
      "Reference",
      "Function reference"
    ]
  },
  {
    "objectID": "notebooks/functions.html",
    "href": "notebooks/functions.html",
    "title": "functions",
    "section": "",
    "text": "``\n\n\n\n\n\nName\nDescription\n\n\n\n\napprox_n_unique\nApproximate count of unique values.\n\n\narctan2\nCompute two argument arctan in radians.\n\n\narctan2d\nCompute two argument arctan in degrees.\n\n\narg_sort_by\nReturn the row indices that would sort the column(s).\n\n\narg_where\nReturn indices where condition evaluates True.\n\n\ncoalesce\nFolds the columns from left to right, keeping the first non-null value.\n\n\ncollect_all\nCollect multiple LazyFrames at the same time.\n\n\ncollect_all_async\nCollect multiple LazyFrames at the same time asynchronously in thread pool.\n\n\ncorr\nCompute the Pearson’s or Spearman rank correlation correlation between two columns.\n\n\ncount\nReturn the number of non-null values in the column.\n\n\ncov\nCompute the covariance between two columns/ expressions.\n\n\ncum_count\nReturn the cumulative count of the non-null values in the column.\n\n\ncum_fold\nCumulatively fold horizontally across columns with a left fold.\n\n\ncum_reduce\nCumulatively reduce horizontally across columns with a left fold.\n\n\nelement\nAlias for an element being evaluated in an eval expression.\n\n\nexclude\nRepresent all columns except for the given columns.\n\n\nfield\nSelect a field in the current struct.with_fields scope.\n\n\nfirst\nGet the first column or value.\n\n\nfold\nAccumulate over multiple columns horizontally/ row wise with a left fold.\n\n\nfrom_epoch\nUtility function that parses an epoch timestamp (or Unix time) to Polars Date(time).\n\n\ngroups\nSyntactic sugar for pl.col(\"foo\").agg_groups().\n\n\nhead\nGet the first n rows.\n\n\nimplode\nAggregate all column values into a list.\n\n\nlast\nGet the last column or value.\n\n\nmap_batches\nMap a custom function over multiple columns/expressions.\n\n\nmap_groups\nApply a custom/user-defined function (UDF) in a GroupBy context.\n\n\nmean\nGet the mean value.\n\n\nmedian\nGet the median value.\n\n\nn_unique\nCount unique values.\n\n\nnth\nGet the nth column(s) of the context.\n\n\nquantile\nSyntactic sugar for pl.col(\"foo\").quantile(..).\n\n\nreduce\nAccumulate over multiple columns horizontally/ row wise with a left fold.\n\n\nrolling_corr\nCompute the rolling correlation between two columns/ expressions.\n\n\nrolling_cov\nCompute the rolling covariance between two columns/ expressions.\n\n\nselect\nRun polars expressions without a context.\n\n\nsql_expr\nParse one or more SQL expressions to Polars expression(s).\n\n\nstd\nGet the standard deviation.\n\n\ntail\nGet the last n rows.\n\n\nvar\nGet the variance.\n\n\n\n\n\napprox_n_unique(*columns)\nApproximate count of unique values.\nThis function is syntactic sugar for pl.col(columns).approx_n_unique(), and uses the HyperLogLog++ algorithm for cardinality estimation.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncolumns\nstr\nOne or more column names.\n()\n\n\n\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 8, 1],\n...         \"b\": [4, 5, 2],\n...         \"c\": [\"foo\", \"bar\", \"foo\"],\n...     }\n... )\n&gt;&gt;&gt; df.select(pl.approx_n_unique(\"a\"))\nshape: (1, 1)\n┌─────┐\n│ a   │\n│ --- │\n│ u32 │\n╞═════╡\n│ 2   │\n└─────┘\n&gt;&gt;&gt; df.select(pl.approx_n_unique(\"b\", \"c\"))\nshape: (1, 2)\n┌─────┬─────┐\n│ b   ┆ c   │\n│ --- ┆ --- │\n│ u32 ┆ u32 │\n╞═════╪═════╡\n│ 3   ┆ 2   │\n└─────┴─────┘\n\n\n\n\narctan2(y, x)\nCompute two argument arctan in radians.\nReturns the angle (in radians) in the plane between the positive x-axis and the ray from the origin to (x,y).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ny\nstr | Expr\nColumn name or Expression.\nrequired\n\n\nx\nstr | Expr\nColumn name or Expression.\nrequired\n\n\n\n\n\n\n&gt;&gt;&gt; c = (2**0.5) / 2\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"y\": [c, -c, c, -c],\n...         \"x\": [c, c, -c, -c],\n...     }\n... )\n&gt;&gt;&gt; df.with_columns(pl.arctan2(\"y\", \"x\").alias(\"atan2\"))\nshape: (4, 3)\n┌───────────┬───────────┬───────────┐\n│ y         ┆ x         ┆ atan2     │\n│ ---       ┆ ---       ┆ ---       │\n│ f64       ┆ f64       ┆ f64       │\n╞═══════════╪═══════════╪═══════════╡\n│ 0.707107  ┆ 0.707107  ┆ 0.785398  │\n│ -0.707107 ┆ 0.707107  ┆ -0.785398 │\n│ 0.707107  ┆ -0.707107 ┆ 2.356194  │\n│ -0.707107 ┆ -0.707107 ┆ -2.356194 │\n└───────────┴───────────┴───────────┘\n\n\n\n\narctan2d(y, x)\nCompute two argument arctan in degrees.\n.. deprecated:: 1.0.0 Use arctan2 followed by :meth:Expr.degrees instead.\nReturns the angle (in degrees) in the plane between the positive x-axis and the ray from the origin to (x,y).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ny\nstr | Expr\nColumn name or Expression.\nrequired\n\n\nx\nstr | Expr\nColumn name or Expression.\nrequired\n\n\n\n\n\n\n&gt;&gt;&gt; c = (2**0.5) / 2\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"y\": [c, -c, c, -c],\n...         \"x\": [c, c, -c, -c],\n...     }\n... )\n&gt;&gt;&gt; df.select(\n...     pl.arctan2d(\"y\", \"x\").alias(\"atan2d\"),\n...     pl.arctan2(\"y\", \"x\").alias(\"atan2\"),\n... )\nshape: (4, 2)\n┌────────┬───────────┐\n│ atan2d ┆ atan2     │\n│ ---    ┆ ---       │\n│ f64    ┆ f64       │\n╞════════╪═══════════╡\n│ 45.0   ┆ 0.785398  │\n│ -45.0  ┆ -0.785398 │\n│ 135.0  ┆ 2.356194  │\n│ -135.0 ┆ -2.356194 │\n└────────┴───────────┘\n\n\n\n\narg_sort_by(exprs, *more_exprs, descending=False, nulls_last=False, multithreaded=True, maintain_order=False)\nReturn the row indices that would sort the column(s).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexprs\nIntoExpr | Iterable[IntoExpr]\nColumn(s) to arg sort by. Accepts expression input. Strings are parsed as column names.\nrequired\n\n\n*more_exprs\nIntoExpr\nAdditional columns to arg sort by, specified as positional arguments.\n()\n\n\ndescending\nbool | Sequence[bool]\nSort in descending order. When sorting by multiple columns, can be specified per column by passing a sequence of booleans.\nFalse\n\n\nnulls_last\nbool | Sequence[bool]\nPlace null values last.\nFalse\n\n\nmultithreaded\nbool\nSort using multiple threads.\nTrue\n\n\nmaintain_order\nbool\nWhether the order should be maintained if elements are equal.\nFalse\n\n\n\n\n\n\nExpr.gather: Take values by index. Expr.rank : Get the rank of each row.\n\n\n\nPass a single column name to compute the arg sort by that column.\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [0, 1, 1, 0],\n...         \"b\": [3, 2, 3, 2],\n...         \"c\": [1, 2, 3, 4],\n...     }\n... )\n&gt;&gt;&gt; df.select(pl.arg_sort_by(\"a\"))\nshape: (4, 1)\n┌─────┐\n│ a   │\n│ --- │\n│ u32 │\n╞═════╡\n│ 0   │\n│ 3   │\n│ 1   │\n│ 2   │\n└─────┘\nCompute the arg sort by multiple columns by either passing a list of columns, or by specifying each column as a positional argument.\n&gt;&gt;&gt; df.select(pl.arg_sort_by([\"a\", \"b\"], descending=True))\nshape: (4, 1)\n┌─────┐\n│ a   │\n│ --- │\n│ u32 │\n╞═════╡\n│ 2   │\n│ 1   │\n│ 0   │\n│ 3   │\n└─────┘\nUse gather to apply the arg sort to other columns.\n&gt;&gt;&gt; df.select(pl.col(\"c\").gather(pl.arg_sort_by(\"a\")))\nshape: (4, 1)\n┌─────┐\n│ c   │\n│ --- │\n│ i64 │\n╞═════╡\n│ 1   │\n│ 4   │\n│ 2   │\n│ 3   │\n└─────┘\n\n\n\n\narg_where(condition, *, eager=False)\nReturn indices where condition evaluates True.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncondition\nExpr | Series\nBoolean expression to evaluate\nrequired\n\n\neager\nbool\nEvaluate immediately and return a Series. If set to False (default), return an expression instead.\nFalse\n\n\n\n\n\n\nSeries.arg_true : Return indices where Series is True\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame({\"a\": [1, 2, 3, 4, 5]})\n&gt;&gt;&gt; df.select(\n...     [\n...         pl.arg_where(pl.col(\"a\") % 2 == 0),\n...     ]\n... ).to_series()\nshape: (2,)\nSeries: 'a' [u32]\n[\n    1\n    3\n]\n\n\n\n\ncoalesce(exprs, *more_exprs)\nFolds the columns from left to right, keeping the first non-null value.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexprs\nIntoExpr | Iterable[IntoExpr]\nColumns to coalesce. Accepts expression input. Strings are parsed as column names, other non-expression inputs are parsed as literals.\nrequired\n\n\n*more_exprs\nIntoExpr\nAdditional columns to coalesce, specified as positional arguments.\n()\n\n\n\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, None, None, None],\n...         \"b\": [1, 2, None, None],\n...         \"c\": [5, None, 3, None],\n...     }\n... )\n&gt;&gt;&gt; df.with_columns(pl.coalesce([\"a\", \"b\", \"c\", 10]).alias(\"d\"))\nshape: (4, 4)\n┌──────┬──────┬──────┬─────┐\n│ a    ┆ b    ┆ c    ┆ d   │\n│ ---  ┆ ---  ┆ ---  ┆ --- │\n│ i64  ┆ i64  ┆ i64  ┆ i64 │\n╞══════╪══════╪══════╪═════╡\n│ 1    ┆ 1    ┆ 5    ┆ 1   │\n│ null ┆ 2    ┆ null ┆ 2   │\n│ null ┆ null ┆ 3    ┆ 3   │\n│ null ┆ null ┆ null ┆ 10  │\n└──────┴──────┴──────┴─────┘\n&gt;&gt;&gt; df.with_columns(pl.coalesce(pl.col([\"a\", \"b\", \"c\"]), 10.0).alias(\"d\"))\nshape: (4, 4)\n┌──────┬──────┬──────┬──────┐\n│ a    ┆ b    ┆ c    ┆ d    │\n│ ---  ┆ ---  ┆ ---  ┆ ---  │\n│ i64  ┆ i64  ┆ i64  ┆ f64  │\n╞══════╪══════╪══════╪══════╡\n│ 1    ┆ 1    ┆ 5    ┆ 1.0  │\n│ null ┆ 2    ┆ null ┆ 2.0  │\n│ null ┆ null ┆ 3    ┆ 3.0  │\n│ null ┆ null ┆ null ┆ 10.0 │\n└──────┴──────┴──────┴──────┘\n\n\n\n\ncollect_all(lazy_frames, *, type_coercion=True, predicate_pushdown=True, projection_pushdown=True, simplify_expression=True, no_optimization=False, slice_pushdown=True, comm_subplan_elim=True, comm_subexpr_elim=True, cluster_with_columns=True, streaming=False)\nCollect multiple LazyFrames at the same time.\nThis runs all the computation graphs in parallel on the Polars threadpool.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlazy_frames\nIterable[LazyFrame]\nA list of LazyFrames to collect.\nrequired\n\n\ntype_coercion\nbool\nDo type coercion optimization.\nTrue\n\n\npredicate_pushdown\nbool\nDo predicate pushdown optimization.\nTrue\n\n\nprojection_pushdown\nbool\nDo projection pushdown optimization.\nTrue\n\n\nsimplify_expression\nbool\nRun simplify expressions optimization.\nTrue\n\n\nno_optimization\nbool\nTurn off optimizations.\nFalse\n\n\nslice_pushdown\nbool\nSlice pushdown optimization.\nTrue\n\n\ncomm_subplan_elim\nbool\nWill try to cache branching subplans that occur on self-joins or unions.\nTrue\n\n\ncomm_subexpr_elim\nbool\nCommon subexpressions will be cached and reused.\nTrue\n\n\ncluster_with_columns\nbool\nCombine sequential independent calls to with_columns\nTrue\n\n\nstreaming\nbool\nProcess the query in batches to handle larger-than-memory data. If set to False (default), the entire query is processed in a single batch. .. warning:: Streaming mode is considered unstable. It may be changed at any point without it being considered a breaking change. .. note:: Use :func:explain to see if Polars can process the query in streaming mode.\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist of DataFrames\nThe collected DataFrames, returned in the same order as the input LazyFrames.\n\n\n\n\n\n\n\ncollect_all_async(lazy_frames, *, gevent=False, type_coercion=True, predicate_pushdown=True, projection_pushdown=True, simplify_expression=True, no_optimization=False, slice_pushdown=True, comm_subplan_elim=True, comm_subexpr_elim=True, cluster_with_columns=True, streaming=False)\nCollect multiple LazyFrames at the same time asynchronously in thread pool.\n.. warning:: This functionality is considered unstable. It may be changed at any point without it being considered a breaking change.\nCollects into a list of DataFrame (like :func:polars.collect_all), but instead of returning them directly, they are scheduled to be collected inside thread pool, while this method returns almost instantly.\nMay be useful if you use gevent or asyncio and want to release control to other greenlets/tasks while LazyFrames are being collected.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlazy_frames\nIterable[LazyFrame]\nA list of LazyFrames to collect.\nrequired\n\n\ngevent\nbool\nReturn wrapper to gevent.event.AsyncResult instead of Awaitable\nFalse\n\n\ntype_coercion\nbool\nDo type coercion optimization.\nTrue\n\n\npredicate_pushdown\nbool\nDo predicate pushdown optimization.\nTrue\n\n\nprojection_pushdown\nbool\nDo projection pushdown optimization.\nTrue\n\n\nsimplify_expression\nbool\nRun simplify expressions optimization.\nTrue\n\n\nno_optimization\nbool\nTurn off (certain) optimizations.\nFalse\n\n\nslice_pushdown\nbool\nSlice pushdown optimization.\nTrue\n\n\ncomm_subplan_elim\nbool\nWill try to cache branching subplans that occur on self-joins or unions.\nTrue\n\n\ncomm_subexpr_elim\nbool\nCommon subexpressions will be cached and reused.\nTrue\n\n\ncluster_with_columns\nbool\nCombine sequential independent calls to with_columns\nTrue\n\n\nstreaming\nbool\nProcess the query in batches to handle larger-than-memory data. If set to False (default), the entire query is processed in a single batch. .. warning:: Streaming mode is considered unstable. It may be changed at any point without it being considered a breaking change. .. note:: Use :func:explain to see if Polars can process the query in streaming mode.\nFalse\n\n\n\n\n\n\npolars.collect_all : Collect multiple LazyFrames at the same time. LazyFrame.collect_async : To collect single frame.\n\n\n\nIn case of error set_exception is used on asyncio.Future/gevent.event.AsyncResult and will be reraised by them.\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nIf gevent=False (default) then returns awaitable.\n\n\n\nIf gevent=True then returns wrapper that has\n\n\n\n.get(block=True, timeout=None) method.\n\n\n\n\n\n\n\n\ncorr(a, b, *, method='pearson', ddof=1, propagate_nans=False)\nCompute the Pearson’s or Spearman rank correlation correlation between two columns.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\na\nIntoExpr\nColumn name or Expression.\nrequired\n\n\nb\nIntoExpr\nColumn name or Expression.\nrequired\n\n\nddof\nint\n“Delta Degrees of Freedom”: the divisor used in the calculation is N - ddof, where N represents the number of elements. By default ddof is 1.\n1\n\n\nmethod\n(‘pearson’, ‘spearman’)\nCorrelation method.\n'pearson'\n\n\npropagate_nans\nbool\nIf True any NaN encountered will lead to NaN in the output. Defaults to False where NaN are regarded as larger than any finite number and thus lead to the highest rank.\nFalse\n\n\n\n\n\n\nPearson’s correlation:\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 8, 3],\n...         \"b\": [4, 5, 2],\n...         \"c\": [\"foo\", \"bar\", \"foo\"],\n...     }\n... )\n&gt;&gt;&gt; df.select(pl.corr(\"a\", \"b\"))\nshape: (1, 1)\n┌──────────┐\n│ a        │\n│ ---      │\n│ f64      │\n╞══════════╡\n│ 0.544705 │\n└──────────┘\nSpearman rank correlation:\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 8, 3],\n...         \"b\": [4, 5, 2],\n...         \"c\": [\"foo\", \"bar\", \"foo\"],\n...     }\n... )\n&gt;&gt;&gt; df.select(pl.corr(\"a\", \"b\", method=\"spearman\"))\nshape: (1, 1)\n┌─────┐\n│ a   │\n│ --- │\n│ f64 │\n╞═════╡\n│ 0.5 │\n└─────┘\n\n\n\n\ncount(*columns)\nReturn the number of non-null values in the column.\nThis function is syntactic sugar for col(columns).count().\nCalling this function without any arguments returns the number of rows in the context. This way of using the function is deprecated. Please use :func:len instead.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n*columns\nstr\nOne or more column names.\n()\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nExpr\nExpression of data type :class:UInt32.\n\n\n\n\n\n\nExpr.count\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 2, None],\n...         \"b\": [3, None, None],\n...         \"c\": [\"foo\", \"bar\", \"foo\"],\n...     }\n... )\n&gt;&gt;&gt; df.select(pl.count(\"a\"))\nshape: (1, 1)\n┌─────┐\n│ a   │\n│ --- │\n│ u32 │\n╞═════╡\n│ 2   │\n└─────┘\nReturn the number of non-null values in multiple columns.\n&gt;&gt;&gt; df.select(pl.count(\"b\", \"c\"))\nshape: (1, 2)\n┌─────┬─────┐\n│ b   ┆ c   │\n│ --- ┆ --- │\n│ u32 ┆ u32 │\n╞═════╪═════╡\n│ 1   ┆ 3   │\n└─────┴─────┘\nReturn the number of rows in a context. This way of using the function is deprecated. Please use :func:len instead.\n&gt;&gt;&gt; df.select(pl.count())\nshape: (1, 1)\n┌───────┐\n│ count │\n│ ---   │\n│ u32   │\n╞═══════╡\n│ 3     │\n└───────┘\n\n\n\n\ncov(a, b, ddof=1)\nCompute the covariance between two columns/ expressions.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\na\nIntoExpr\nColumn name or Expression.\nrequired\n\n\nb\nIntoExpr\nColumn name or Expression.\nrequired\n\n\nddof\nint\n“Delta Degrees of Freedom”: the divisor used in the calculation is N - ddof, where N represents the number of elements. By default ddof is 1.\n1\n\n\n\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 8, 3],\n...         \"b\": [4, 5, 2],\n...         \"c\": [\"foo\", \"bar\", \"foo\"],\n...     },\n... )\n&gt;&gt;&gt; df.select(pl.cov(\"a\", \"b\"))\nshape: (1, 1)\n┌─────┐\n│ a   │\n│ --- │\n│ f64 │\n╞═════╡\n│ 3.0 │\n└─────┘\n\n\n\n\ncum_count(*columns, reverse=False)\nReturn the cumulative count of the non-null values in the column.\nThis function is syntactic sugar for col(columns).cum_count().\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n*columns\nstr\nName(s) of the columns to use.\n()\n\n\nreverse\nbool\nReverse the operation.\nFalse\n\n\n\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame({\"a\": [1, 2, None], \"b\": [3, None, None]})\n&gt;&gt;&gt; df.select(pl.cum_count(\"a\"))\nshape: (3, 1)\n┌─────┐\n│ a   │\n│ --- │\n│ u32 │\n╞═════╡\n│ 1   │\n│ 2   │\n│ 2   │\n└─────┘\n\n\n\n\ncum_fold(acc, function, exprs, *, include_init=False)\nCumulatively fold horizontally across columns with a left fold.\nEvery cumulative result is added as a separate field in a Struct column.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nacc\nIntoExpr\nAccumulator expression. This is the value that will be initialized when the fold starts. For a sum this could for instance be lit(0).\nrequired\n\n\nfunction\nCallable[[Series, Series], Series]\nFunction to apply over the accumulator and the value. Fn(acc, value) -&gt; new_value\nrequired\n\n\nexprs\nSequence[Expr | str] | Expr\nExpressions to aggregate over. May also be a wildcard expression.\nrequired\n\n\ninclude_init\nbool\nInclude the initial accumulator state as struct field.\nFalse\n\n\n\n\n\n\nIf you simply want the first encountered expression as accumulator, consider using :func:cum_reduce.\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 2, 3],\n...         \"b\": [3, 4, 5],\n...         \"c\": [5, 6, 7],\n...     }\n... )\n&gt;&gt;&gt; df.with_columns(\n...     pl.cum_fold(acc=pl.lit(1), function=lambda acc, x: acc + x, exprs=pl.all())\n... )\nshape: (3, 4)\n┌─────┬─────┬─────┬───────────┐\n│ a   ┆ b   ┆ c   ┆ cum_fold  │\n│ --- ┆ --- ┆ --- ┆ ---       │\n│ i64 ┆ i64 ┆ i64 ┆ struct[3] │\n╞═════╪═════╪═════╪═══════════╡\n│ 1   ┆ 3   ┆ 5   ┆ {2,5,10}  │\n│ 2   ┆ 4   ┆ 6   ┆ {3,7,13}  │\n│ 3   ┆ 5   ┆ 7   ┆ {4,9,16}  │\n└─────┴─────┴─────┴───────────┘\n\n\n\n\ncum_reduce(function, exprs)\nCumulatively reduce horizontally across columns with a left fold.\nEvery cumulative result is added as a separate field in a Struct column.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunction\nCallable[[Series, Series], Series]\nFunction to apply over the accumulator and the value. Fn(acc, value) -&gt; new_value\nrequired\n\n\nexprs\nSequence[Expr | str] | Expr\nExpressions to aggregate over. May also be a wildcard expression.\nrequired\n\n\n\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 2, 3],\n...         \"b\": [3, 4, 5],\n...         \"c\": [5, 6, 7],\n...     }\n... )\n&gt;&gt;&gt; df.with_columns(pl.cum_reduce(function=lambda acc, x: acc + x, exprs=pl.all()))\nshape: (3, 4)\n┌─────┬─────┬─────┬────────────┐\n│ a   ┆ b   ┆ c   ┆ cum_reduce │\n│ --- ┆ --- ┆ --- ┆ ---        │\n│ i64 ┆ i64 ┆ i64 ┆ struct[3]  │\n╞═════╪═════╪═════╪════════════╡\n│ 1   ┆ 3   ┆ 5   ┆ {1,4,9}    │\n│ 2   ┆ 4   ┆ 6   ┆ {2,6,12}   │\n│ 3   ┆ 5   ┆ 7   ┆ {3,8,15}   │\n└─────┴─────┴─────┴────────────┘\n\n\n\n\nelement()\nAlias for an element being evaluated in an eval expression.\n\n\nA horizontal rank computation by taking the elements of a list\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 8, 3],\n...         \"b\": [4, 5, 2],\n...     }\n... )\n&gt;&gt;&gt; df.with_columns(\n...     pl.concat_list([\"a\", \"b\"]).list.eval(pl.element().rank()).alias(\"rank\")\n... )\nshape: (3, 3)\n┌─────┬─────┬────────────┐\n│ a   ┆ b   ┆ rank       │\n│ --- ┆ --- ┆ ---        │\n│ i64 ┆ i64 ┆ list[f64]  │\n╞═════╪═════╪════════════╡\n│ 1   ┆ 4   ┆ [1.0, 2.0] │\n│ 8   ┆ 5   ┆ [2.0, 1.0] │\n│ 3   ┆ 2   ┆ [2.0, 1.0] │\n└─────┴─────┴────────────┘\nA mathematical operation on array elements\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 8, 3],\n...         \"b\": [4, 5, 2],\n...     }\n... )\n&gt;&gt;&gt; df.with_columns(\n...     pl.concat_list([\"a\", \"b\"]).list.eval(pl.element() * 2).alias(\"a_b_doubled\")\n... )\nshape: (3, 3)\n┌─────┬─────┬─────────────┐\n│ a   ┆ b   ┆ a_b_doubled │\n│ --- ┆ --- ┆ ---         │\n│ i64 ┆ i64 ┆ list[i64]   │\n╞═════╪═════╪═════════════╡\n│ 1   ┆ 4   ┆ [2, 8]      │\n│ 8   ┆ 5   ┆ [16, 10]    │\n│ 3   ┆ 2   ┆ [6, 4]      │\n└─────┴─────┴─────────────┘\n\n\n\n\nexclude(columns, *more_columns)\nRepresent all columns except for the given columns.\nSyntactic sugar for pl.all().exclude(columns).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncolumns\nstr | PolarsDataType | Collection[str] | Collection[PolarsDataType]\nThe name or datatype of the column(s) to exclude. Accepts regular expression input. Regular expressions should start with ^ and end with $.\nrequired\n\n\n*more_columns\nstr | PolarsDataType\nAdditional names or datatypes of columns to exclude, specified as positional arguments.\n()\n\n\n\n\n\n\nExclude by column name(s):\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"aa\": [1, 2, 3],\n...         \"ba\": [\"a\", \"b\", None],\n...         \"cc\": [None, 2.5, 1.5],\n...     }\n... )\n&gt;&gt;&gt; df.select(pl.exclude(\"ba\"))\nshape: (3, 2)\n┌─────┬──────┐\n│ aa  ┆ cc   │\n│ --- ┆ ---  │\n│ i64 ┆ f64  │\n╞═════╪══════╡\n│ 1   ┆ null │\n│ 2   ┆ 2.5  │\n│ 3   ┆ 1.5  │\n└─────┴──────┘\nExclude by regex, e.g. removing all columns whose names end with the letter “a”:\n&gt;&gt;&gt; df.select(pl.exclude(\"^.*a$\"))\nshape: (3, 1)\n┌──────┐\n│ cc   │\n│ ---  │\n│ f64  │\n╞══════╡\n│ null │\n│ 2.5  │\n│ 1.5  │\n└──────┘\nExclude by dtype(s), e.g. removing all columns of type Int64 or Float64:\n&gt;&gt;&gt; df.select(pl.exclude([pl.Int64, pl.Float64]))\nshape: (3, 1)\n┌──────┐\n│ ba   │\n│ ---  │\n│ str  │\n╞══════╡\n│ a    │\n│ b    │\n│ null │\n└──────┘\n\n\n\n\nfield(name)\nSelect a field in the current struct.with_fields scope.\nname Name of the field(s) to select.\n\n\n\nfirst(*columns)\nGet the first column or value.\nThis function has different behavior depending on the presence of columns values. If none given (the default), returns an expression that takes the first column of the context; otherwise, takes the first value of the given column(s).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n*columns\nstr\nOne or more column names.\n()\n\n\n\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 8, 3],\n...         \"b\": [4, 5, 2],\n...         \"c\": [\"foo\", \"bar\", \"baz\"],\n...     }\n... )\nReturn the first column:\n&gt;&gt;&gt; df.select(pl.first())\nshape: (3, 1)\n┌─────┐\n│ a   │\n│ --- │\n│ i64 │\n╞═════╡\n│ 1   │\n│ 8   │\n│ 3   │\n└─────┘\nReturn the first value for the given column(s):\n&gt;&gt;&gt; df.select(pl.first(\"b\"))\nshape: (1, 1)\n┌─────┐\n│ b   │\n│ --- │\n│ i64 │\n╞═════╡\n│ 4   │\n└─────┘\n&gt;&gt;&gt; df.select(pl.first(\"a\", \"c\"))\nshape: (1, 2)\n┌─────┬─────┐\n│ a   ┆ c   │\n│ --- ┆ --- │\n│ i64 ┆ str │\n╞═════╪═════╡\n│ 1   ┆ foo │\n└─────┴─────┘\n\n\n\n\nfold(acc, function, exprs)\nAccumulate over multiple columns horizontally/ row wise with a left fold.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nacc\nIntoExpr\nAccumulator Expression. This is the value that will be initialized when the fold starts. For a sum this could for instance be lit(0).\nrequired\n\n\nfunction\nCallable[[Series, Series], Series]\nFunction to apply over the accumulator and the value. Fn(acc, value) -&gt; new_value\nrequired\n\n\nexprs\nSequence[Expr | str] | Expr\nExpressions to aggregate over. May also be a wildcard expression.\nrequired\n\n\n\n\n\n\nIf you simply want the first encountered expression as accumulator, consider using reduce.\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 2, 3],\n...         \"b\": [3, 4, 5],\n...         \"c\": [5, 6, 7],\n...     }\n... )\n&gt;&gt;&gt; df\nshape: (3, 3)\n┌─────┬─────┬─────┐\n│ a   ┆ b   ┆ c   │\n│ --- ┆ --- ┆ --- │\n│ i64 ┆ i64 ┆ i64 │\n╞═════╪═════╪═════╡\n│ 1   ┆ 3   ┆ 5   │\n│ 2   ┆ 4   ┆ 6   │\n│ 3   ┆ 5   ┆ 7   │\n└─────┴─────┴─────┘\nHorizontally sum over all columns and add 1.\n&gt;&gt;&gt; df.select(\n...     pl.fold(\n...         acc=pl.lit(1), function=lambda acc, x: acc + x, exprs=pl.col(\"*\")\n...     ).alias(\"sum\"),\n... )\nshape: (3, 1)\n┌─────┐\n│ sum │\n│ --- │\n│ i64 │\n╞═════╡\n│ 10  │\n│ 13  │\n│ 16  │\n└─────┘\nYou can also apply a condition/predicate on all columns:\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 2, 3],\n...         \"b\": [0, 1, 2],\n...     }\n... )\n&gt;&gt;&gt; df\nshape: (3, 2)\n┌─────┬─────┐\n│ a   ┆ b   │\n│ --- ┆ --- │\n│ i64 ┆ i64 │\n╞═════╪═════╡\n│ 1   ┆ 0   │\n│ 2   ┆ 1   │\n│ 3   ┆ 2   │\n└─────┴─────┘\n&gt;&gt;&gt; df.filter(\n...     pl.fold(\n...         acc=pl.lit(True),\n...         function=lambda acc, x: acc & x,\n...         exprs=pl.col(\"*\") &gt; 1,\n...     )\n... )\nshape: (1, 2)\n┌─────┬─────┐\n│ a   ┆ b   │\n│ --- ┆ --- │\n│ i64 ┆ i64 │\n╞═════╪═════╡\n│ 3   ┆ 2   │\n└─────┴─────┘\n\n\n\n\nfrom_epoch(column, time_unit='s')\nUtility function that parses an epoch timestamp (or Unix time) to Polars Date(time).\nDepending on the time_unit provided, this function will return a different dtype:\n\ntime_unit=“d” returns pl.Date\ntime_unit=“s” returns pl.Datetime[“us”] (pl.Datetime’s default)\ntime_unit=“ms” returns pl.Datetime[“ms”]\ntime_unit=“us” returns pl.Datetime[“us”]\ntime_unit=“ns” returns pl.Datetime[“ns”]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncolumn\nstr | Expr | Series | Sequence[int]\nSeries or expression to parse integers to pl.Datetime.\nrequired\n\n\ntime_unit\nEpochTimeUnit\nThe unit of time of the timesteps since epoch time.\n's'\n\n\n\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame({\"timestamp\": [1666683077, 1666683099]}).lazy()\n&gt;&gt;&gt; df.select(pl.from_epoch(pl.col(\"timestamp\"), time_unit=\"s\")).collect()\nshape: (2, 1)\n┌─────────────────────┐\n│ timestamp           │\n│ ---                 │\n│ datetime[μs]        │\n╞═════════════════════╡\n│ 2022-10-25 07:31:17 │\n│ 2022-10-25 07:31:39 │\n└─────────────────────┘\nThe function can also be used in an eager context by passing a Series.\n&gt;&gt;&gt; s = pl.Series([12345, 12346])\n&gt;&gt;&gt; pl.from_epoch(s, time_unit=\"d\")\nshape: (2,)\nSeries: '' [date]\n[\n        2003-10-20\n        2003-10-21\n]\n\n\n\n\ngroups(column)\nSyntactic sugar for pl.col(\"foo\").agg_groups().\n\n\n\nhead(column, n=10)\nGet the first n rows.\nThis function is syntactic sugar for pl.col(column).head(n).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncolumn\nstr\nColumn name.\nrequired\n\n\nn\nint\nNumber of rows to return.\n10\n\n\n\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 8, 3],\n...         \"b\": [4, 5, 2],\n...         \"c\": [\"foo\", \"bar\", \"foo\"],\n...     }\n... )\n&gt;&gt;&gt; df.select(pl.head(\"a\"))\nshape: (3, 1)\n┌─────┐\n│ a   │\n│ --- │\n│ i64 │\n╞═════╡\n│ 1   │\n│ 8   │\n│ 3   │\n└─────┘\n&gt;&gt;&gt; df.select(pl.head(\"a\", 2))\nshape: (2, 1)\n┌─────┐\n│ a   │\n│ --- │\n│ i64 │\n╞═════╡\n│ 1   │\n│ 8   │\n└─────┘\n\n\n\n\nimplode(*columns)\nAggregate all column values into a list.\nThis function is syntactic sugar for pl.col(name).implode().\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n*columns\nstr\nOne or more column names.\n()\n\n\n\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 2, 3],\n...         \"b\": [9, 8, 7],\n...         \"c\": [\"foo\", \"bar\", \"foo\"],\n...     }\n... )\n&gt;&gt;&gt; df.select(pl.implode(\"a\"))\nshape: (1, 1)\n┌───────────┐\n│ a         │\n│ ---       │\n│ list[i64] │\n╞═══════════╡\n│ [1, 2, 3] │\n└───────────┘\n&gt;&gt;&gt; df.select(pl.implode(\"b\", \"c\"))\nshape: (1, 2)\n┌───────────┬───────────────────────┐\n│ b         ┆ c                     │\n│ ---       ┆ ---                   │\n│ list[i64] ┆ list[str]             │\n╞═══════════╪═══════════════════════╡\n│ [9, 8, 7] ┆ [\"foo\", \"bar\", \"foo\"] │\n└───────────┴───────────────────────┘\n\n\n\n\nlast(*columns)\nGet the last column or value.\nThis function has different behavior depending on the presence of columns values. If none given (the default), returns an expression that takes the last column of the context; otherwise, takes the last value of the given column(s).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n*columns\nstr\nOne or more column names.\n()\n\n\n\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 8, 3],\n...         \"b\": [4, 5, 2],\n...         \"c\": [\"foo\", \"bar\", \"baz\"],\n...     }\n... )\nReturn the last column:\n&gt;&gt;&gt; df.select(pl.last())\nshape: (3, 1)\n┌─────┐\n│ c   │\n│ --- │\n│ str │\n╞═════╡\n│ foo │\n│ bar │\n│ baz │\n└─────┘\nReturn the last value for the given column(s):\n&gt;&gt;&gt; df.select(pl.last(\"a\"))\nshape: (1, 1)\n┌─────┐\n│ a   │\n│ --- │\n│ i64 │\n╞═════╡\n│ 3   │\n└─────┘\n&gt;&gt;&gt; df.select(pl.last(\"b\", \"c\"))\nshape: (1, 2)\n┌─────┬─────┐\n│ b   ┆ c   │\n│ --- ┆ --- │\n│ i64 ┆ str │\n╞═════╪═════╡\n│ 2   ┆ baz │\n└─────┴─────┘\n\n\n\n\nmap_batches(exprs, function, return_dtype=None)\nMap a custom function over multiple columns/expressions.\nProduces a single Series result.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexprs\nSequence[str] | Sequence[Expr]\nExpression(s) representing the input Series to the function.\nrequired\n\n\nfunction\nCallable[[Sequence[Series]], Series]\nFunction to apply over the input.\nrequired\n\n\nreturn_dtype\nPolarsDataType | None\ndtype of the output Series.\nNone\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nExpr\nExpression with the data type given by return_dtype.\n\n\n\n\n\n\n&gt;&gt;&gt; def test_func(a, b, c):\n...     return a + b + c\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 2, 3, 4],\n...         \"b\": [4, 5, 6, 7],\n...     }\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; df.with_columns(\n...     (\n...         pl.struct([\"a\", \"b\"]).map_batches(\n...             lambda x: test_func(x.struct.field(\"a\"), x.struct.field(\"b\"), 1)\n...         )\n...     ).alias(\"a+b+c\")\n... )\nshape: (4, 3)\n┌─────┬─────┬───────┐\n│ a   ┆ b   ┆ a+b+c │\n│ --- ┆ --- ┆ ---   │\n│ i64 ┆ i64 ┆ i64   │\n╞═════╪═════╪═══════╡\n│ 1   ┆ 4   ┆ 6     │\n│ 2   ┆ 5   ┆ 8     │\n│ 3   ┆ 6   ┆ 10    │\n│ 4   ┆ 7   ┆ 12    │\n└─────┴─────┴───────┘\n\n\n\n\nmap_groups(exprs, function, return_dtype=None, *, returns_scalar=True)\nApply a custom/user-defined function (UDF) in a GroupBy context.\n.. warning:: This method is much slower than the native expressions API. Only use it if you cannot implement your logic otherwise.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexprs\nSequence[str | Expr]\nExpression(s) representing the input Series to the function.\nrequired\n\n\nfunction\nCallable[[Sequence[Series]], Series | Any]\nFunction to apply over the input; should be of type Callable[[Series], Series].\nrequired\n\n\nreturn_dtype\nPolarsDataType | None\ndtype of the output Series.\nNone\n\n\nreturns_scalar\nbool\nIf the function returns a single scalar as output.\nTrue\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nExpr\nExpression with the data type given by return_dtype.\n\n\n\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"group\": [1, 1, 2],\n...         \"a\": [1, 3, 3],\n...         \"b\": [5, 6, 7],\n...     }\n... )\n&gt;&gt;&gt; df\nshape: (3, 3)\n┌───────┬─────┬─────┐\n│ group ┆ a   ┆ b   │\n│ ---   ┆ --- ┆ --- │\n│ i64   ┆ i64 ┆ i64 │\n╞═══════╪═════╪═════╡\n│ 1     ┆ 1   ┆ 5   │\n│ 1     ┆ 3   ┆ 6   │\n│ 2     ┆ 3   ┆ 7   │\n└───────┴─────┴─────┘\n&gt;&gt;&gt; (\n...     df.group_by(\"group\").agg(\n...         pl.map_groups(\n...             exprs=[\"a\", \"b\"],\n...             function=lambda list_of_series: list_of_series[0]\n...             / list_of_series[0].sum()\n...             + list_of_series[1],\n...         ).alias(\"my_custom_aggregation\")\n...     )\n... ).sort(\"group\")\nshape: (2, 2)\n┌───────┬───────────────────────┐\n│ group ┆ my_custom_aggregation │\n│ ---   ┆ ---                   │\n│ i64   ┆ list[f64]             │\n╞═══════╪═══════════════════════╡\n│ 1     ┆ [5.25, 6.75]          │\n│ 2     ┆ [8.0]                 │\n└───────┴───────────────────────┘\nThe output for group 1 can be understood as follows:\n\ngroup 1 contains Series 'a': [1, 3] and 'b': [4, 5]\napplying the function to those lists of Series, one gets the output [1 / 4 + 5, 3 / 4 + 6], i.e. [5.25, 6.75]\n\n\n\n\n\nmean(*columns)\nGet the mean value.\nThis function is syntactic sugar for pl.col(columns).mean().\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n*columns\nstr\nOne or more column names.\n()\n\n\n\n\n\n\nmean_horizontal\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 8, 3],\n...         \"b\": [4, 5, 2],\n...         \"c\": [\"foo\", \"bar\", \"foo\"],\n...     }\n... )\n&gt;&gt;&gt; df.select(pl.mean(\"a\"))\nshape: (1, 1)\n┌─────┐\n│ a   │\n│ --- │\n│ f64 │\n╞═════╡\n│ 4.0 │\n└─────┘\n&gt;&gt;&gt; df.select(pl.mean(\"a\", \"b\"))\nshape: (1, 2)\n┌─────┬──────────┐\n│ a   ┆ b        │\n│ --- ┆ ---      │\n│ f64 ┆ f64      │\n╞═════╪══════════╡\n│ 4.0 ┆ 3.666667 │\n└─────┴──────────┘\n\n\n\n\nmedian(*columns)\nGet the median value.\nThis function is syntactic sugar for pl.col(columns).median().\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncolumns\nstr\nOne or more column names.\n()\n\n\n\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 8, 3],\n...         \"b\": [4, 5, 2],\n...         \"c\": [\"foo\", \"bar\", \"foo\"],\n...     }\n... )\n&gt;&gt;&gt; df.select(pl.median(\"a\"))\nshape: (1, 1)\n┌─────┐\n│ a   │\n│ --- │\n│ f64 │\n╞═════╡\n│ 3.0 │\n└─────┘\n&gt;&gt;&gt; df.select(pl.median(\"a\", \"b\"))\nshape: (1, 2)\n┌─────┬─────┐\n│ a   ┆ b   │\n│ --- ┆ --- │\n│ f64 ┆ f64 │\n╞═════╪═════╡\n│ 3.0 ┆ 4.0 │\n└─────┴─────┘\n\n\n\n\nn_unique(*columns)\nCount unique values.\nThis function is syntactic sugar for pl.col(columns).n_unique().\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncolumns\nstr\nOne or more column names.\n()\n\n\n\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 8, 1],\n...         \"b\": [4, 5, 2],\n...         \"c\": [\"foo\", \"bar\", \"foo\"],\n...     }\n... )\n&gt;&gt;&gt; df.select(pl.n_unique(\"a\"))\nshape: (1, 1)\n┌─────┐\n│ a   │\n│ --- │\n│ u32 │\n╞═════╡\n│ 2   │\n└─────┘\n&gt;&gt;&gt; df.select(pl.n_unique(\"b\", \"c\"))\nshape: (1, 2)\n┌─────┬─────┐\n│ b   ┆ c   │\n│ --- ┆ --- │\n│ u32 ┆ u32 │\n╞═════╪═════╡\n│ 3   ┆ 2   │\n└─────┴─────┘\n\n\n\n\nnth(*indices)\nGet the nth column(s) of the context.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nindices\nint | Sequence[int]\nOne or more indices representing the columns to retrieve.\n()\n\n\n\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 8, 3],\n...         \"b\": [4, 5, 2],\n...         \"c\": [\"foo\", \"bar\", \"baz\"],\n...     }\n... )\n&gt;&gt;&gt; df.select(pl.nth(1))\nshape: (3, 1)\n┌─────┐\n│ b   │\n│ --- │\n│ i64 │\n╞═════╡\n│ 4   │\n│ 5   │\n│ 2   │\n└─────┘\n&gt;&gt;&gt; df.select(pl.nth(2, 0))\nshape: (3, 2)\n┌─────┬─────┐\n│ c   ┆ a   │\n│ --- ┆ --- │\n│ str ┆ i64 │\n╞═════╪═════╡\n│ foo ┆ 1   │\n│ bar ┆ 8   │\n│ baz ┆ 3   │\n└─────┴─────┘\n\n\n\n\nquantile(column, quantile, interpolation='nearest')\nSyntactic sugar for pl.col(\"foo\").quantile(..).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncolumn\nstr\nColumn name.\nrequired\n\n\nquantile\nfloat | Expr\nQuantile between 0.0 and 1.0.\nrequired\n\n\ninterpolation\n(‘nearest’, ‘higher’, ‘lower’, ‘midpoint’, ‘linear’)\nInterpolation method.\n'nearest'\n\n\n\n\n\n\n\nreduce(function, exprs)\nAccumulate over multiple columns horizontally/ row wise with a left fold.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunction\nCallable[[Series, Series], Series]\nFunction to apply over the accumulator and the value. Fn(acc, value) -&gt; new_value\nrequired\n\n\nexprs\nSequence[Expr | str] | Expr\nExpressions to aggregate over. May also be a wildcard expression.\nrequired\n\n\n\n\n\n\nSee fold for the version with an explicit accumulator.\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 2, 3],\n...         \"b\": [0, 1, 2],\n...     }\n... )\n&gt;&gt;&gt; df\nshape: (3, 2)\n┌─────┬─────┐\n│ a   ┆ b   │\n│ --- ┆ --- │\n│ i64 ┆ i64 │\n╞═════╪═════╡\n│ 1   ┆ 0   │\n│ 2   ┆ 1   │\n│ 3   ┆ 2   │\n└─────┴─────┘\nHorizontally sum over all columns.\n&gt;&gt;&gt; df.select(\n...     pl.reduce(function=lambda acc, x: acc + x, exprs=pl.col(\"*\")).alias(\"sum\")\n... )\nshape: (3, 1)\n┌─────┐\n│ sum │\n│ --- │\n│ i64 │\n╞═════╡\n│ 1   │\n│ 3   │\n│ 5   │\n└─────┘\n\n\n\n\nrolling_corr(a, b, *, window_size, min_periods=None, ddof=1)\nCompute the rolling correlation between two columns/ expressions.\n.. warning:: This functionality is considered unstable. It may be changed at any point without it being considered a breaking change.\nThe window at a given row includes the row itself and the window_size - 1 elements before it.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\na\nstr | Expr\nColumn name or Expression.\nrequired\n\n\nb\nstr | Expr\nColumn name or Expression.\nrequired\n\n\nwindow_size\nint\nThe length of the window.\nrequired\n\n\nmin_periods\nint | None\nThe number of values in the window that should be non-null before computing a result. If None, it will be set equal to window size.\nNone\n\n\nddof\nint\nDelta degrees of freedom. The divisor used in calculations is N - ddof, where N represents the number of elements.\n1\n\n\n\n\n\n\n\nrolling_cov(a, b, *, window_size, min_periods=None, ddof=1)\nCompute the rolling covariance between two columns/ expressions.\n.. warning:: This functionality is considered unstable. It may be changed at any point without it being considered a breaking change.\nThe window at a given row includes the row itself and the window_size - 1 elements before it.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\na\nstr | Expr\nColumn name or Expression.\nrequired\n\n\nb\nstr | Expr\nColumn name or Expression.\nrequired\n\n\nwindow_size\nint\nThe length of the window.\nrequired\n\n\nmin_periods\nint | None\nThe number of values in the window that should be non-null before computing a result. If None, it will be set equal to window size.\nNone\n\n\nddof\nint\nDelta degrees of freedom. The divisor used in calculations is N - ddof, where N represents the number of elements.\n1\n\n\n\n\n\n\n\nselect(*exprs, **named_exprs)\nRun polars expressions without a context.\nThis is syntactic sugar for running df.select on an empty DataFrame.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n*exprs\nIntoExpr | Iterable[IntoExpr]\nColumn(s) to select, specified as positional arguments. Accepts expression input. Strings are parsed as column names, other non-expression inputs are parsed as literals.\n()\n\n\n**named_exprs\nIntoExpr\nAdditional columns to select, specified as keyword arguments. The columns will be renamed to the keyword used.\n{}\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nDataFrame\n\n\n\n\n\n\n\n&gt;&gt;&gt; foo = pl.Series(\"foo\", [1, 2, 3])\n&gt;&gt;&gt; bar = pl.Series(\"bar\", [3, 2, 1])\n&gt;&gt;&gt; pl.select(min=pl.min_horizontal(foo, bar))\nshape: (3, 1)\n┌─────┐\n│ min │\n│ --- │\n│ i64 │\n╞═════╡\n│ 1   │\n│ 2   │\n│ 1   │\n└─────┘\n\n\n\n\nsql_expr(sql)\nParse one or more SQL expressions to Polars expression(s).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsql\nstr | Sequence[str]\nOne or more SQL expressions.\nrequired\n\n\n\n\n\n\nParse a single SQL expression:\n&gt;&gt;&gt; df = pl.DataFrame({\"a\": [2, 1]})\n&gt;&gt;&gt; expr = pl.sql_expr(\"MAX(a)\")\n&gt;&gt;&gt; df.select(expr)\nshape: (1, 1)\n┌─────┐\n│ a   │\n│ --- │\n│ i64 │\n╞═════╡\n│ 2   │\n└─────┘\nParse multiple SQL expressions:\n&gt;&gt;&gt; df.with_columns(\n...     *pl.sql_expr([\"POWER(a,a) AS a_a\", \"CAST(a AS TEXT) AS a_txt\"]),\n... )\nshape: (2, 3)\n┌─────┬─────┬───────┐\n│ a   ┆ a_a ┆ a_txt │\n│ --- ┆ --- ┆ ---   │\n│ i64 ┆ i64 ┆ str   │\n╞═════╪═════╪═══════╡\n│ 2   ┆ 4   ┆ 2     │\n│ 1   ┆ 1   ┆ 1     │\n└─────┴─────┴───────┘\n\n\n\n\nstd(column, ddof=1)\nGet the standard deviation.\nThis function is syntactic sugar for pl.col(column).std(ddof).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncolumn\nstr\nColumn name.\nrequired\n\n\nddof\nint\n“Delta Degrees of Freedom”: the divisor used in the calculation is N - ddof, where N represents the number of elements. By default ddof is 1.\n1\n\n\n\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 8, 3],\n...         \"b\": [4, 5, 2],\n...         \"c\": [\"foo\", \"bar\", \"foo\"],\n...     }\n... )\n&gt;&gt;&gt; df.select(pl.std(\"a\"))\nshape: (1, 1)\n┌──────────┐\n│ a        │\n│ ---      │\n│ f64      │\n╞══════════╡\n│ 3.605551 │\n└──────────┘\n&gt;&gt;&gt; df[\"a\"].std()\n3.605551275463989\n\n\n\n\ntail(column, n=10)\nGet the last n rows.\nThis function is syntactic sugar for pl.col(column).tail(n).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncolumn\nstr\nColumn name.\nrequired\n\n\nn\nint\nNumber of rows to return.\n10\n\n\n\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 8, 3],\n...         \"b\": [4, 5, 2],\n...         \"c\": [\"foo\", \"bar\", \"foo\"],\n...     }\n... )\n&gt;&gt;&gt; df.select(pl.tail(\"a\"))\nshape: (3, 1)\n┌─────┐\n│ a   │\n│ --- │\n│ i64 │\n╞═════╡\n│ 1   │\n│ 8   │\n│ 3   │\n└─────┘\n&gt;&gt;&gt; df.select(pl.tail(\"a\", 2))\nshape: (2, 1)\n┌─────┐\n│ a   │\n│ --- │\n│ i64 │\n╞═════╡\n│ 8   │\n│ 3   │\n└─────┘\n\n\n\n\nvar(column, ddof=1)\nGet the variance.\nThis function is syntactic sugar for pl.col(column).var(ddof).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncolumn\nstr\nColumn name.\nrequired\n\n\nddof\nint\n“Delta Degrees of Freedom”: the divisor used in the calculation is N - ddof, where N represents the number of elements. By default ddof is 1.\n1\n\n\n\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 8, 3],\n...         \"b\": [4, 5, 2],\n...         \"c\": [\"foo\", \"bar\", \"foo\"],\n...     },\n... )\n&gt;&gt;&gt; df.select(pl.var(\"a\"))\nshape: (1, 1)\n┌──────┐\n│ a    │\n│ ---  │\n│ f64  │\n╞══════╡\n│ 13.0 │\n└──────┘\n&gt;&gt;&gt; df[\"a\"].var()\n13.0",
    "crumbs": [
      "Reference",
      "Main Functions",
      "functions"
    ]
  },
  {
    "objectID": "notebooks/functions.html#functions-1",
    "href": "notebooks/functions.html#functions-1",
    "title": "functions",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\napprox_n_unique\nApproximate count of unique values.\n\n\narctan2\nCompute two argument arctan in radians.\n\n\narctan2d\nCompute two argument arctan in degrees.\n\n\narg_sort_by\nReturn the row indices that would sort the column(s).\n\n\narg_where\nReturn indices where condition evaluates True.\n\n\ncoalesce\nFolds the columns from left to right, keeping the first non-null value.\n\n\ncollect_all\nCollect multiple LazyFrames at the same time.\n\n\ncollect_all_async\nCollect multiple LazyFrames at the same time asynchronously in thread pool.\n\n\ncorr\nCompute the Pearson’s or Spearman rank correlation correlation between two columns.\n\n\ncount\nReturn the number of non-null values in the column.\n\n\ncov\nCompute the covariance between two columns/ expressions.\n\n\ncum_count\nReturn the cumulative count of the non-null values in the column.\n\n\ncum_fold\nCumulatively fold horizontally across columns with a left fold.\n\n\ncum_reduce\nCumulatively reduce horizontally across columns with a left fold.\n\n\nelement\nAlias for an element being evaluated in an eval expression.\n\n\nexclude\nRepresent all columns except for the given columns.\n\n\nfield\nSelect a field in the current struct.with_fields scope.\n\n\nfirst\nGet the first column or value.\n\n\nfold\nAccumulate over multiple columns horizontally/ row wise with a left fold.\n\n\nfrom_epoch\nUtility function that parses an epoch timestamp (or Unix time) to Polars Date(time).\n\n\ngroups\nSyntactic sugar for pl.col(\"foo\").agg_groups().\n\n\nhead\nGet the first n rows.\n\n\nimplode\nAggregate all column values into a list.\n\n\nlast\nGet the last column or value.\n\n\nmap_batches\nMap a custom function over multiple columns/expressions.\n\n\nmap_groups\nApply a custom/user-defined function (UDF) in a GroupBy context.\n\n\nmean\nGet the mean value.\n\n\nmedian\nGet the median value.\n\n\nn_unique\nCount unique values.\n\n\nnth\nGet the nth column(s) of the context.\n\n\nquantile\nSyntactic sugar for pl.col(\"foo\").quantile(..).\n\n\nreduce\nAccumulate over multiple columns horizontally/ row wise with a left fold.\n\n\nrolling_corr\nCompute the rolling correlation between two columns/ expressions.\n\n\nrolling_cov\nCompute the rolling covariance between two columns/ expressions.\n\n\nselect\nRun polars expressions without a context.\n\n\nsql_expr\nParse one or more SQL expressions to Polars expression(s).\n\n\nstd\nGet the standard deviation.\n\n\ntail\nGet the last n rows.\n\n\nvar\nGet the variance.\n\n\n\n\n\napprox_n_unique(*columns)\nApproximate count of unique values.\nThis function is syntactic sugar for pl.col(columns).approx_n_unique(), and uses the HyperLogLog++ algorithm for cardinality estimation.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncolumns\nstr\nOne or more column names.\n()\n\n\n\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 8, 1],\n...         \"b\": [4, 5, 2],\n...         \"c\": [\"foo\", \"bar\", \"foo\"],\n...     }\n... )\n&gt;&gt;&gt; df.select(pl.approx_n_unique(\"a\"))\nshape: (1, 1)\n┌─────┐\n│ a   │\n│ --- │\n│ u32 │\n╞═════╡\n│ 2   │\n└─────┘\n&gt;&gt;&gt; df.select(pl.approx_n_unique(\"b\", \"c\"))\nshape: (1, 2)\n┌─────┬─────┐\n│ b   ┆ c   │\n│ --- ┆ --- │\n│ u32 ┆ u32 │\n╞═════╪═════╡\n│ 3   ┆ 2   │\n└─────┴─────┘\n\n\n\n\narctan2(y, x)\nCompute two argument arctan in radians.\nReturns the angle (in radians) in the plane between the positive x-axis and the ray from the origin to (x,y).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ny\nstr | Expr\nColumn name or Expression.\nrequired\n\n\nx\nstr | Expr\nColumn name or Expression.\nrequired\n\n\n\n\n\n\n&gt;&gt;&gt; c = (2**0.5) / 2\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"y\": [c, -c, c, -c],\n...         \"x\": [c, c, -c, -c],\n...     }\n... )\n&gt;&gt;&gt; df.with_columns(pl.arctan2(\"y\", \"x\").alias(\"atan2\"))\nshape: (4, 3)\n┌───────────┬───────────┬───────────┐\n│ y         ┆ x         ┆ atan2     │\n│ ---       ┆ ---       ┆ ---       │\n│ f64       ┆ f64       ┆ f64       │\n╞═══════════╪═══════════╪═══════════╡\n│ 0.707107  ┆ 0.707107  ┆ 0.785398  │\n│ -0.707107 ┆ 0.707107  ┆ -0.785398 │\n│ 0.707107  ┆ -0.707107 ┆ 2.356194  │\n│ -0.707107 ┆ -0.707107 ┆ -2.356194 │\n└───────────┴───────────┴───────────┘\n\n\n\n\narctan2d(y, x)\nCompute two argument arctan in degrees.\n.. deprecated:: 1.0.0 Use arctan2 followed by :meth:Expr.degrees instead.\nReturns the angle (in degrees) in the plane between the positive x-axis and the ray from the origin to (x,y).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ny\nstr | Expr\nColumn name or Expression.\nrequired\n\n\nx\nstr | Expr\nColumn name or Expression.\nrequired\n\n\n\n\n\n\n&gt;&gt;&gt; c = (2**0.5) / 2\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"y\": [c, -c, c, -c],\n...         \"x\": [c, c, -c, -c],\n...     }\n... )\n&gt;&gt;&gt; df.select(\n...     pl.arctan2d(\"y\", \"x\").alias(\"atan2d\"),\n...     pl.arctan2(\"y\", \"x\").alias(\"atan2\"),\n... )\nshape: (4, 2)\n┌────────┬───────────┐\n│ atan2d ┆ atan2     │\n│ ---    ┆ ---       │\n│ f64    ┆ f64       │\n╞════════╪═══════════╡\n│ 45.0   ┆ 0.785398  │\n│ -45.0  ┆ -0.785398 │\n│ 135.0  ┆ 2.356194  │\n│ -135.0 ┆ -2.356194 │\n└────────┴───────────┘\n\n\n\n\narg_sort_by(exprs, *more_exprs, descending=False, nulls_last=False, multithreaded=True, maintain_order=False)\nReturn the row indices that would sort the column(s).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexprs\nIntoExpr | Iterable[IntoExpr]\nColumn(s) to arg sort by. Accepts expression input. Strings are parsed as column names.\nrequired\n\n\n*more_exprs\nIntoExpr\nAdditional columns to arg sort by, specified as positional arguments.\n()\n\n\ndescending\nbool | Sequence[bool]\nSort in descending order. When sorting by multiple columns, can be specified per column by passing a sequence of booleans.\nFalse\n\n\nnulls_last\nbool | Sequence[bool]\nPlace null values last.\nFalse\n\n\nmultithreaded\nbool\nSort using multiple threads.\nTrue\n\n\nmaintain_order\nbool\nWhether the order should be maintained if elements are equal.\nFalse\n\n\n\n\n\n\nExpr.gather: Take values by index. Expr.rank : Get the rank of each row.\n\n\n\nPass a single column name to compute the arg sort by that column.\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [0, 1, 1, 0],\n...         \"b\": [3, 2, 3, 2],\n...         \"c\": [1, 2, 3, 4],\n...     }\n... )\n&gt;&gt;&gt; df.select(pl.arg_sort_by(\"a\"))\nshape: (4, 1)\n┌─────┐\n│ a   │\n│ --- │\n│ u32 │\n╞═════╡\n│ 0   │\n│ 3   │\n│ 1   │\n│ 2   │\n└─────┘\nCompute the arg sort by multiple columns by either passing a list of columns, or by specifying each column as a positional argument.\n&gt;&gt;&gt; df.select(pl.arg_sort_by([\"a\", \"b\"], descending=True))\nshape: (4, 1)\n┌─────┐\n│ a   │\n│ --- │\n│ u32 │\n╞═════╡\n│ 2   │\n│ 1   │\n│ 0   │\n│ 3   │\n└─────┘\nUse gather to apply the arg sort to other columns.\n&gt;&gt;&gt; df.select(pl.col(\"c\").gather(pl.arg_sort_by(\"a\")))\nshape: (4, 1)\n┌─────┐\n│ c   │\n│ --- │\n│ i64 │\n╞═════╡\n│ 1   │\n│ 4   │\n│ 2   │\n│ 3   │\n└─────┘\n\n\n\n\narg_where(condition, *, eager=False)\nReturn indices where condition evaluates True.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncondition\nExpr | Series\nBoolean expression to evaluate\nrequired\n\n\neager\nbool\nEvaluate immediately and return a Series. If set to False (default), return an expression instead.\nFalse\n\n\n\n\n\n\nSeries.arg_true : Return indices where Series is True\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame({\"a\": [1, 2, 3, 4, 5]})\n&gt;&gt;&gt; df.select(\n...     [\n...         pl.arg_where(pl.col(\"a\") % 2 == 0),\n...     ]\n... ).to_series()\nshape: (2,)\nSeries: 'a' [u32]\n[\n    1\n    3\n]\n\n\n\n\ncoalesce(exprs, *more_exprs)\nFolds the columns from left to right, keeping the first non-null value.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexprs\nIntoExpr | Iterable[IntoExpr]\nColumns to coalesce. Accepts expression input. Strings are parsed as column names, other non-expression inputs are parsed as literals.\nrequired\n\n\n*more_exprs\nIntoExpr\nAdditional columns to coalesce, specified as positional arguments.\n()\n\n\n\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, None, None, None],\n...         \"b\": [1, 2, None, None],\n...         \"c\": [5, None, 3, None],\n...     }\n... )\n&gt;&gt;&gt; df.with_columns(pl.coalesce([\"a\", \"b\", \"c\", 10]).alias(\"d\"))\nshape: (4, 4)\n┌──────┬──────┬──────┬─────┐\n│ a    ┆ b    ┆ c    ┆ d   │\n│ ---  ┆ ---  ┆ ---  ┆ --- │\n│ i64  ┆ i64  ┆ i64  ┆ i64 │\n╞══════╪══════╪══════╪═════╡\n│ 1    ┆ 1    ┆ 5    ┆ 1   │\n│ null ┆ 2    ┆ null ┆ 2   │\n│ null ┆ null ┆ 3    ┆ 3   │\n│ null ┆ null ┆ null ┆ 10  │\n└──────┴──────┴──────┴─────┘\n&gt;&gt;&gt; df.with_columns(pl.coalesce(pl.col([\"a\", \"b\", \"c\"]), 10.0).alias(\"d\"))\nshape: (4, 4)\n┌──────┬──────┬──────┬──────┐\n│ a    ┆ b    ┆ c    ┆ d    │\n│ ---  ┆ ---  ┆ ---  ┆ ---  │\n│ i64  ┆ i64  ┆ i64  ┆ f64  │\n╞══════╪══════╪══════╪══════╡\n│ 1    ┆ 1    ┆ 5    ┆ 1.0  │\n│ null ┆ 2    ┆ null ┆ 2.0  │\n│ null ┆ null ┆ 3    ┆ 3.0  │\n│ null ┆ null ┆ null ┆ 10.0 │\n└──────┴──────┴──────┴──────┘\n\n\n\n\ncollect_all(lazy_frames, *, type_coercion=True, predicate_pushdown=True, projection_pushdown=True, simplify_expression=True, no_optimization=False, slice_pushdown=True, comm_subplan_elim=True, comm_subexpr_elim=True, cluster_with_columns=True, streaming=False)\nCollect multiple LazyFrames at the same time.\nThis runs all the computation graphs in parallel on the Polars threadpool.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlazy_frames\nIterable[LazyFrame]\nA list of LazyFrames to collect.\nrequired\n\n\ntype_coercion\nbool\nDo type coercion optimization.\nTrue\n\n\npredicate_pushdown\nbool\nDo predicate pushdown optimization.\nTrue\n\n\nprojection_pushdown\nbool\nDo projection pushdown optimization.\nTrue\n\n\nsimplify_expression\nbool\nRun simplify expressions optimization.\nTrue\n\n\nno_optimization\nbool\nTurn off optimizations.\nFalse\n\n\nslice_pushdown\nbool\nSlice pushdown optimization.\nTrue\n\n\ncomm_subplan_elim\nbool\nWill try to cache branching subplans that occur on self-joins or unions.\nTrue\n\n\ncomm_subexpr_elim\nbool\nCommon subexpressions will be cached and reused.\nTrue\n\n\ncluster_with_columns\nbool\nCombine sequential independent calls to with_columns\nTrue\n\n\nstreaming\nbool\nProcess the query in batches to handle larger-than-memory data. If set to False (default), the entire query is processed in a single batch. .. warning:: Streaming mode is considered unstable. It may be changed at any point without it being considered a breaking change. .. note:: Use :func:explain to see if Polars can process the query in streaming mode.\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist of DataFrames\nThe collected DataFrames, returned in the same order as the input LazyFrames.\n\n\n\n\n\n\n\ncollect_all_async(lazy_frames, *, gevent=False, type_coercion=True, predicate_pushdown=True, projection_pushdown=True, simplify_expression=True, no_optimization=False, slice_pushdown=True, comm_subplan_elim=True, comm_subexpr_elim=True, cluster_with_columns=True, streaming=False)\nCollect multiple LazyFrames at the same time asynchronously in thread pool.\n.. warning:: This functionality is considered unstable. It may be changed at any point without it being considered a breaking change.\nCollects into a list of DataFrame (like :func:polars.collect_all), but instead of returning them directly, they are scheduled to be collected inside thread pool, while this method returns almost instantly.\nMay be useful if you use gevent or asyncio and want to release control to other greenlets/tasks while LazyFrames are being collected.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlazy_frames\nIterable[LazyFrame]\nA list of LazyFrames to collect.\nrequired\n\n\ngevent\nbool\nReturn wrapper to gevent.event.AsyncResult instead of Awaitable\nFalse\n\n\ntype_coercion\nbool\nDo type coercion optimization.\nTrue\n\n\npredicate_pushdown\nbool\nDo predicate pushdown optimization.\nTrue\n\n\nprojection_pushdown\nbool\nDo projection pushdown optimization.\nTrue\n\n\nsimplify_expression\nbool\nRun simplify expressions optimization.\nTrue\n\n\nno_optimization\nbool\nTurn off (certain) optimizations.\nFalse\n\n\nslice_pushdown\nbool\nSlice pushdown optimization.\nTrue\n\n\ncomm_subplan_elim\nbool\nWill try to cache branching subplans that occur on self-joins or unions.\nTrue\n\n\ncomm_subexpr_elim\nbool\nCommon subexpressions will be cached and reused.\nTrue\n\n\ncluster_with_columns\nbool\nCombine sequential independent calls to with_columns\nTrue\n\n\nstreaming\nbool\nProcess the query in batches to handle larger-than-memory data. If set to False (default), the entire query is processed in a single batch. .. warning:: Streaming mode is considered unstable. It may be changed at any point without it being considered a breaking change. .. note:: Use :func:explain to see if Polars can process the query in streaming mode.\nFalse\n\n\n\n\n\n\npolars.collect_all : Collect multiple LazyFrames at the same time. LazyFrame.collect_async : To collect single frame.\n\n\n\nIn case of error set_exception is used on asyncio.Future/gevent.event.AsyncResult and will be reraised by them.\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nIf gevent=False (default) then returns awaitable.\n\n\n\nIf gevent=True then returns wrapper that has\n\n\n\n.get(block=True, timeout=None) method.\n\n\n\n\n\n\n\n\ncorr(a, b, *, method='pearson', ddof=1, propagate_nans=False)\nCompute the Pearson’s or Spearman rank correlation correlation between two columns.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\na\nIntoExpr\nColumn name or Expression.\nrequired\n\n\nb\nIntoExpr\nColumn name or Expression.\nrequired\n\n\nddof\nint\n“Delta Degrees of Freedom”: the divisor used in the calculation is N - ddof, where N represents the number of elements. By default ddof is 1.\n1\n\n\nmethod\n(‘pearson’, ‘spearman’)\nCorrelation method.\n'pearson'\n\n\npropagate_nans\nbool\nIf True any NaN encountered will lead to NaN in the output. Defaults to False where NaN are regarded as larger than any finite number and thus lead to the highest rank.\nFalse\n\n\n\n\n\n\nPearson’s correlation:\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 8, 3],\n...         \"b\": [4, 5, 2],\n...         \"c\": [\"foo\", \"bar\", \"foo\"],\n...     }\n... )\n&gt;&gt;&gt; df.select(pl.corr(\"a\", \"b\"))\nshape: (1, 1)\n┌──────────┐\n│ a        │\n│ ---      │\n│ f64      │\n╞══════════╡\n│ 0.544705 │\n└──────────┘\nSpearman rank correlation:\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 8, 3],\n...         \"b\": [4, 5, 2],\n...         \"c\": [\"foo\", \"bar\", \"foo\"],\n...     }\n... )\n&gt;&gt;&gt; df.select(pl.corr(\"a\", \"b\", method=\"spearman\"))\nshape: (1, 1)\n┌─────┐\n│ a   │\n│ --- │\n│ f64 │\n╞═════╡\n│ 0.5 │\n└─────┘\n\n\n\n\ncount(*columns)\nReturn the number of non-null values in the column.\nThis function is syntactic sugar for col(columns).count().\nCalling this function without any arguments returns the number of rows in the context. This way of using the function is deprecated. Please use :func:len instead.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n*columns\nstr\nOne or more column names.\n()\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nExpr\nExpression of data type :class:UInt32.\n\n\n\n\n\n\nExpr.count\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 2, None],\n...         \"b\": [3, None, None],\n...         \"c\": [\"foo\", \"bar\", \"foo\"],\n...     }\n... )\n&gt;&gt;&gt; df.select(pl.count(\"a\"))\nshape: (1, 1)\n┌─────┐\n│ a   │\n│ --- │\n│ u32 │\n╞═════╡\n│ 2   │\n└─────┘\nReturn the number of non-null values in multiple columns.\n&gt;&gt;&gt; df.select(pl.count(\"b\", \"c\"))\nshape: (1, 2)\n┌─────┬─────┐\n│ b   ┆ c   │\n│ --- ┆ --- │\n│ u32 ┆ u32 │\n╞═════╪═════╡\n│ 1   ┆ 3   │\n└─────┴─────┘\nReturn the number of rows in a context. This way of using the function is deprecated. Please use :func:len instead.\n&gt;&gt;&gt; df.select(pl.count())\nshape: (1, 1)\n┌───────┐\n│ count │\n│ ---   │\n│ u32   │\n╞═══════╡\n│ 3     │\n└───────┘\n\n\n\n\ncov(a, b, ddof=1)\nCompute the covariance between two columns/ expressions.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\na\nIntoExpr\nColumn name or Expression.\nrequired\n\n\nb\nIntoExpr\nColumn name or Expression.\nrequired\n\n\nddof\nint\n“Delta Degrees of Freedom”: the divisor used in the calculation is N - ddof, where N represents the number of elements. By default ddof is 1.\n1\n\n\n\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 8, 3],\n...         \"b\": [4, 5, 2],\n...         \"c\": [\"foo\", \"bar\", \"foo\"],\n...     },\n... )\n&gt;&gt;&gt; df.select(pl.cov(\"a\", \"b\"))\nshape: (1, 1)\n┌─────┐\n│ a   │\n│ --- │\n│ f64 │\n╞═════╡\n│ 3.0 │\n└─────┘\n\n\n\n\ncum_count(*columns, reverse=False)\nReturn the cumulative count of the non-null values in the column.\nThis function is syntactic sugar for col(columns).cum_count().\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n*columns\nstr\nName(s) of the columns to use.\n()\n\n\nreverse\nbool\nReverse the operation.\nFalse\n\n\n\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame({\"a\": [1, 2, None], \"b\": [3, None, None]})\n&gt;&gt;&gt; df.select(pl.cum_count(\"a\"))\nshape: (3, 1)\n┌─────┐\n│ a   │\n│ --- │\n│ u32 │\n╞═════╡\n│ 1   │\n│ 2   │\n│ 2   │\n└─────┘\n\n\n\n\ncum_fold(acc, function, exprs, *, include_init=False)\nCumulatively fold horizontally across columns with a left fold.\nEvery cumulative result is added as a separate field in a Struct column.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nacc\nIntoExpr\nAccumulator expression. This is the value that will be initialized when the fold starts. For a sum this could for instance be lit(0).\nrequired\n\n\nfunction\nCallable[[Series, Series], Series]\nFunction to apply over the accumulator and the value. Fn(acc, value) -&gt; new_value\nrequired\n\n\nexprs\nSequence[Expr | str] | Expr\nExpressions to aggregate over. May also be a wildcard expression.\nrequired\n\n\ninclude_init\nbool\nInclude the initial accumulator state as struct field.\nFalse\n\n\n\n\n\n\nIf you simply want the first encountered expression as accumulator, consider using :func:cum_reduce.\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 2, 3],\n...         \"b\": [3, 4, 5],\n...         \"c\": [5, 6, 7],\n...     }\n... )\n&gt;&gt;&gt; df.with_columns(\n...     pl.cum_fold(acc=pl.lit(1), function=lambda acc, x: acc + x, exprs=pl.all())\n... )\nshape: (3, 4)\n┌─────┬─────┬─────┬───────────┐\n│ a   ┆ b   ┆ c   ┆ cum_fold  │\n│ --- ┆ --- ┆ --- ┆ ---       │\n│ i64 ┆ i64 ┆ i64 ┆ struct[3] │\n╞═════╪═════╪═════╪═══════════╡\n│ 1   ┆ 3   ┆ 5   ┆ {2,5,10}  │\n│ 2   ┆ 4   ┆ 6   ┆ {3,7,13}  │\n│ 3   ┆ 5   ┆ 7   ┆ {4,9,16}  │\n└─────┴─────┴─────┴───────────┘\n\n\n\n\ncum_reduce(function, exprs)\nCumulatively reduce horizontally across columns with a left fold.\nEvery cumulative result is added as a separate field in a Struct column.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunction\nCallable[[Series, Series], Series]\nFunction to apply over the accumulator and the value. Fn(acc, value) -&gt; new_value\nrequired\n\n\nexprs\nSequence[Expr | str] | Expr\nExpressions to aggregate over. May also be a wildcard expression.\nrequired\n\n\n\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 2, 3],\n...         \"b\": [3, 4, 5],\n...         \"c\": [5, 6, 7],\n...     }\n... )\n&gt;&gt;&gt; df.with_columns(pl.cum_reduce(function=lambda acc, x: acc + x, exprs=pl.all()))\nshape: (3, 4)\n┌─────┬─────┬─────┬────────────┐\n│ a   ┆ b   ┆ c   ┆ cum_reduce │\n│ --- ┆ --- ┆ --- ┆ ---        │\n│ i64 ┆ i64 ┆ i64 ┆ struct[3]  │\n╞═════╪═════╪═════╪════════════╡\n│ 1   ┆ 3   ┆ 5   ┆ {1,4,9}    │\n│ 2   ┆ 4   ┆ 6   ┆ {2,6,12}   │\n│ 3   ┆ 5   ┆ 7   ┆ {3,8,15}   │\n└─────┴─────┴─────┴────────────┘\n\n\n\n\nelement()\nAlias for an element being evaluated in an eval expression.\n\n\nA horizontal rank computation by taking the elements of a list\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 8, 3],\n...         \"b\": [4, 5, 2],\n...     }\n... )\n&gt;&gt;&gt; df.with_columns(\n...     pl.concat_list([\"a\", \"b\"]).list.eval(pl.element().rank()).alias(\"rank\")\n... )\nshape: (3, 3)\n┌─────┬─────┬────────────┐\n│ a   ┆ b   ┆ rank       │\n│ --- ┆ --- ┆ ---        │\n│ i64 ┆ i64 ┆ list[f64]  │\n╞═════╪═════╪════════════╡\n│ 1   ┆ 4   ┆ [1.0, 2.0] │\n│ 8   ┆ 5   ┆ [2.0, 1.0] │\n│ 3   ┆ 2   ┆ [2.0, 1.0] │\n└─────┴─────┴────────────┘\nA mathematical operation on array elements\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 8, 3],\n...         \"b\": [4, 5, 2],\n...     }\n... )\n&gt;&gt;&gt; df.with_columns(\n...     pl.concat_list([\"a\", \"b\"]).list.eval(pl.element() * 2).alias(\"a_b_doubled\")\n... )\nshape: (3, 3)\n┌─────┬─────┬─────────────┐\n│ a   ┆ b   ┆ a_b_doubled │\n│ --- ┆ --- ┆ ---         │\n│ i64 ┆ i64 ┆ list[i64]   │\n╞═════╪═════╪═════════════╡\n│ 1   ┆ 4   ┆ [2, 8]      │\n│ 8   ┆ 5   ┆ [16, 10]    │\n│ 3   ┆ 2   ┆ [6, 4]      │\n└─────┴─────┴─────────────┘\n\n\n\n\nexclude(columns, *more_columns)\nRepresent all columns except for the given columns.\nSyntactic sugar for pl.all().exclude(columns).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncolumns\nstr | PolarsDataType | Collection[str] | Collection[PolarsDataType]\nThe name or datatype of the column(s) to exclude. Accepts regular expression input. Regular expressions should start with ^ and end with $.\nrequired\n\n\n*more_columns\nstr | PolarsDataType\nAdditional names or datatypes of columns to exclude, specified as positional arguments.\n()\n\n\n\n\n\n\nExclude by column name(s):\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"aa\": [1, 2, 3],\n...         \"ba\": [\"a\", \"b\", None],\n...         \"cc\": [None, 2.5, 1.5],\n...     }\n... )\n&gt;&gt;&gt; df.select(pl.exclude(\"ba\"))\nshape: (3, 2)\n┌─────┬──────┐\n│ aa  ┆ cc   │\n│ --- ┆ ---  │\n│ i64 ┆ f64  │\n╞═════╪══════╡\n│ 1   ┆ null │\n│ 2   ┆ 2.5  │\n│ 3   ┆ 1.5  │\n└─────┴──────┘\nExclude by regex, e.g. removing all columns whose names end with the letter “a”:\n&gt;&gt;&gt; df.select(pl.exclude(\"^.*a$\"))\nshape: (3, 1)\n┌──────┐\n│ cc   │\n│ ---  │\n│ f64  │\n╞══════╡\n│ null │\n│ 2.5  │\n│ 1.5  │\n└──────┘\nExclude by dtype(s), e.g. removing all columns of type Int64 or Float64:\n&gt;&gt;&gt; df.select(pl.exclude([pl.Int64, pl.Float64]))\nshape: (3, 1)\n┌──────┐\n│ ba   │\n│ ---  │\n│ str  │\n╞══════╡\n│ a    │\n│ b    │\n│ null │\n└──────┘\n\n\n\n\nfield(name)\nSelect a field in the current struct.with_fields scope.\nname Name of the field(s) to select.\n\n\n\nfirst(*columns)\nGet the first column or value.\nThis function has different behavior depending on the presence of columns values. If none given (the default), returns an expression that takes the first column of the context; otherwise, takes the first value of the given column(s).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n*columns\nstr\nOne or more column names.\n()\n\n\n\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 8, 3],\n...         \"b\": [4, 5, 2],\n...         \"c\": [\"foo\", \"bar\", \"baz\"],\n...     }\n... )\nReturn the first column:\n&gt;&gt;&gt; df.select(pl.first())\nshape: (3, 1)\n┌─────┐\n│ a   │\n│ --- │\n│ i64 │\n╞═════╡\n│ 1   │\n│ 8   │\n│ 3   │\n└─────┘\nReturn the first value for the given column(s):\n&gt;&gt;&gt; df.select(pl.first(\"b\"))\nshape: (1, 1)\n┌─────┐\n│ b   │\n│ --- │\n│ i64 │\n╞═════╡\n│ 4   │\n└─────┘\n&gt;&gt;&gt; df.select(pl.first(\"a\", \"c\"))\nshape: (1, 2)\n┌─────┬─────┐\n│ a   ┆ c   │\n│ --- ┆ --- │\n│ i64 ┆ str │\n╞═════╪═════╡\n│ 1   ┆ foo │\n└─────┴─────┘\n\n\n\n\nfold(acc, function, exprs)\nAccumulate over multiple columns horizontally/ row wise with a left fold.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nacc\nIntoExpr\nAccumulator Expression. This is the value that will be initialized when the fold starts. For a sum this could for instance be lit(0).\nrequired\n\n\nfunction\nCallable[[Series, Series], Series]\nFunction to apply over the accumulator and the value. Fn(acc, value) -&gt; new_value\nrequired\n\n\nexprs\nSequence[Expr | str] | Expr\nExpressions to aggregate over. May also be a wildcard expression.\nrequired\n\n\n\n\n\n\nIf you simply want the first encountered expression as accumulator, consider using reduce.\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 2, 3],\n...         \"b\": [3, 4, 5],\n...         \"c\": [5, 6, 7],\n...     }\n... )\n&gt;&gt;&gt; df\nshape: (3, 3)\n┌─────┬─────┬─────┐\n│ a   ┆ b   ┆ c   │\n│ --- ┆ --- ┆ --- │\n│ i64 ┆ i64 ┆ i64 │\n╞═════╪═════╪═════╡\n│ 1   ┆ 3   ┆ 5   │\n│ 2   ┆ 4   ┆ 6   │\n│ 3   ┆ 5   ┆ 7   │\n└─────┴─────┴─────┘\nHorizontally sum over all columns and add 1.\n&gt;&gt;&gt; df.select(\n...     pl.fold(\n...         acc=pl.lit(1), function=lambda acc, x: acc + x, exprs=pl.col(\"*\")\n...     ).alias(\"sum\"),\n... )\nshape: (3, 1)\n┌─────┐\n│ sum │\n│ --- │\n│ i64 │\n╞═════╡\n│ 10  │\n│ 13  │\n│ 16  │\n└─────┘\nYou can also apply a condition/predicate on all columns:\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 2, 3],\n...         \"b\": [0, 1, 2],\n...     }\n... )\n&gt;&gt;&gt; df\nshape: (3, 2)\n┌─────┬─────┐\n│ a   ┆ b   │\n│ --- ┆ --- │\n│ i64 ┆ i64 │\n╞═════╪═════╡\n│ 1   ┆ 0   │\n│ 2   ┆ 1   │\n│ 3   ┆ 2   │\n└─────┴─────┘\n&gt;&gt;&gt; df.filter(\n...     pl.fold(\n...         acc=pl.lit(True),\n...         function=lambda acc, x: acc & x,\n...         exprs=pl.col(\"*\") &gt; 1,\n...     )\n... )\nshape: (1, 2)\n┌─────┬─────┐\n│ a   ┆ b   │\n│ --- ┆ --- │\n│ i64 ┆ i64 │\n╞═════╪═════╡\n│ 3   ┆ 2   │\n└─────┴─────┘\n\n\n\n\nfrom_epoch(column, time_unit='s')\nUtility function that parses an epoch timestamp (or Unix time) to Polars Date(time).\nDepending on the time_unit provided, this function will return a different dtype:\n\ntime_unit=“d” returns pl.Date\ntime_unit=“s” returns pl.Datetime[“us”] (pl.Datetime’s default)\ntime_unit=“ms” returns pl.Datetime[“ms”]\ntime_unit=“us” returns pl.Datetime[“us”]\ntime_unit=“ns” returns pl.Datetime[“ns”]\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncolumn\nstr | Expr | Series | Sequence[int]\nSeries or expression to parse integers to pl.Datetime.\nrequired\n\n\ntime_unit\nEpochTimeUnit\nThe unit of time of the timesteps since epoch time.\n's'\n\n\n\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame({\"timestamp\": [1666683077, 1666683099]}).lazy()\n&gt;&gt;&gt; df.select(pl.from_epoch(pl.col(\"timestamp\"), time_unit=\"s\")).collect()\nshape: (2, 1)\n┌─────────────────────┐\n│ timestamp           │\n│ ---                 │\n│ datetime[μs]        │\n╞═════════════════════╡\n│ 2022-10-25 07:31:17 │\n│ 2022-10-25 07:31:39 │\n└─────────────────────┘\nThe function can also be used in an eager context by passing a Series.\n&gt;&gt;&gt; s = pl.Series([12345, 12346])\n&gt;&gt;&gt; pl.from_epoch(s, time_unit=\"d\")\nshape: (2,)\nSeries: '' [date]\n[\n        2003-10-20\n        2003-10-21\n]\n\n\n\n\ngroups(column)\nSyntactic sugar for pl.col(\"foo\").agg_groups().\n\n\n\nhead(column, n=10)\nGet the first n rows.\nThis function is syntactic sugar for pl.col(column).head(n).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncolumn\nstr\nColumn name.\nrequired\n\n\nn\nint\nNumber of rows to return.\n10\n\n\n\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 8, 3],\n...         \"b\": [4, 5, 2],\n...         \"c\": [\"foo\", \"bar\", \"foo\"],\n...     }\n... )\n&gt;&gt;&gt; df.select(pl.head(\"a\"))\nshape: (3, 1)\n┌─────┐\n│ a   │\n│ --- │\n│ i64 │\n╞═════╡\n│ 1   │\n│ 8   │\n│ 3   │\n└─────┘\n&gt;&gt;&gt; df.select(pl.head(\"a\", 2))\nshape: (2, 1)\n┌─────┐\n│ a   │\n│ --- │\n│ i64 │\n╞═════╡\n│ 1   │\n│ 8   │\n└─────┘\n\n\n\n\nimplode(*columns)\nAggregate all column values into a list.\nThis function is syntactic sugar for pl.col(name).implode().\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n*columns\nstr\nOne or more column names.\n()\n\n\n\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 2, 3],\n...         \"b\": [9, 8, 7],\n...         \"c\": [\"foo\", \"bar\", \"foo\"],\n...     }\n... )\n&gt;&gt;&gt; df.select(pl.implode(\"a\"))\nshape: (1, 1)\n┌───────────┐\n│ a         │\n│ ---       │\n│ list[i64] │\n╞═══════════╡\n│ [1, 2, 3] │\n└───────────┘\n&gt;&gt;&gt; df.select(pl.implode(\"b\", \"c\"))\nshape: (1, 2)\n┌───────────┬───────────────────────┐\n│ b         ┆ c                     │\n│ ---       ┆ ---                   │\n│ list[i64] ┆ list[str]             │\n╞═══════════╪═══════════════════════╡\n│ [9, 8, 7] ┆ [\"foo\", \"bar\", \"foo\"] │\n└───────────┴───────────────────────┘\n\n\n\n\nlast(*columns)\nGet the last column or value.\nThis function has different behavior depending on the presence of columns values. If none given (the default), returns an expression that takes the last column of the context; otherwise, takes the last value of the given column(s).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n*columns\nstr\nOne or more column names.\n()\n\n\n\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 8, 3],\n...         \"b\": [4, 5, 2],\n...         \"c\": [\"foo\", \"bar\", \"baz\"],\n...     }\n... )\nReturn the last column:\n&gt;&gt;&gt; df.select(pl.last())\nshape: (3, 1)\n┌─────┐\n│ c   │\n│ --- │\n│ str │\n╞═════╡\n│ foo │\n│ bar │\n│ baz │\n└─────┘\nReturn the last value for the given column(s):\n&gt;&gt;&gt; df.select(pl.last(\"a\"))\nshape: (1, 1)\n┌─────┐\n│ a   │\n│ --- │\n│ i64 │\n╞═════╡\n│ 3   │\n└─────┘\n&gt;&gt;&gt; df.select(pl.last(\"b\", \"c\"))\nshape: (1, 2)\n┌─────┬─────┐\n│ b   ┆ c   │\n│ --- ┆ --- │\n│ i64 ┆ str │\n╞═════╪═════╡\n│ 2   ┆ baz │\n└─────┴─────┘\n\n\n\n\nmap_batches(exprs, function, return_dtype=None)\nMap a custom function over multiple columns/expressions.\nProduces a single Series result.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexprs\nSequence[str] | Sequence[Expr]\nExpression(s) representing the input Series to the function.\nrequired\n\n\nfunction\nCallable[[Sequence[Series]], Series]\nFunction to apply over the input.\nrequired\n\n\nreturn_dtype\nPolarsDataType | None\ndtype of the output Series.\nNone\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nExpr\nExpression with the data type given by return_dtype.\n\n\n\n\n\n\n&gt;&gt;&gt; def test_func(a, b, c):\n...     return a + b + c\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 2, 3, 4],\n...         \"b\": [4, 5, 6, 7],\n...     }\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; df.with_columns(\n...     (\n...         pl.struct([\"a\", \"b\"]).map_batches(\n...             lambda x: test_func(x.struct.field(\"a\"), x.struct.field(\"b\"), 1)\n...         )\n...     ).alias(\"a+b+c\")\n... )\nshape: (4, 3)\n┌─────┬─────┬───────┐\n│ a   ┆ b   ┆ a+b+c │\n│ --- ┆ --- ┆ ---   │\n│ i64 ┆ i64 ┆ i64   │\n╞═════╪═════╪═══════╡\n│ 1   ┆ 4   ┆ 6     │\n│ 2   ┆ 5   ┆ 8     │\n│ 3   ┆ 6   ┆ 10    │\n│ 4   ┆ 7   ┆ 12    │\n└─────┴─────┴───────┘\n\n\n\n\nmap_groups(exprs, function, return_dtype=None, *, returns_scalar=True)\nApply a custom/user-defined function (UDF) in a GroupBy context.\n.. warning:: This method is much slower than the native expressions API. Only use it if you cannot implement your logic otherwise.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexprs\nSequence[str | Expr]\nExpression(s) representing the input Series to the function.\nrequired\n\n\nfunction\nCallable[[Sequence[Series]], Series | Any]\nFunction to apply over the input; should be of type Callable[[Series], Series].\nrequired\n\n\nreturn_dtype\nPolarsDataType | None\ndtype of the output Series.\nNone\n\n\nreturns_scalar\nbool\nIf the function returns a single scalar as output.\nTrue\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nExpr\nExpression with the data type given by return_dtype.\n\n\n\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"group\": [1, 1, 2],\n...         \"a\": [1, 3, 3],\n...         \"b\": [5, 6, 7],\n...     }\n... )\n&gt;&gt;&gt; df\nshape: (3, 3)\n┌───────┬─────┬─────┐\n│ group ┆ a   ┆ b   │\n│ ---   ┆ --- ┆ --- │\n│ i64   ┆ i64 ┆ i64 │\n╞═══════╪═════╪═════╡\n│ 1     ┆ 1   ┆ 5   │\n│ 1     ┆ 3   ┆ 6   │\n│ 2     ┆ 3   ┆ 7   │\n└───────┴─────┴─────┘\n&gt;&gt;&gt; (\n...     df.group_by(\"group\").agg(\n...         pl.map_groups(\n...             exprs=[\"a\", \"b\"],\n...             function=lambda list_of_series: list_of_series[0]\n...             / list_of_series[0].sum()\n...             + list_of_series[1],\n...         ).alias(\"my_custom_aggregation\")\n...     )\n... ).sort(\"group\")\nshape: (2, 2)\n┌───────┬───────────────────────┐\n│ group ┆ my_custom_aggregation │\n│ ---   ┆ ---                   │\n│ i64   ┆ list[f64]             │\n╞═══════╪═══════════════════════╡\n│ 1     ┆ [5.25, 6.75]          │\n│ 2     ┆ [8.0]                 │\n└───────┴───────────────────────┘\nThe output for group 1 can be understood as follows:\n\ngroup 1 contains Series 'a': [1, 3] and 'b': [4, 5]\napplying the function to those lists of Series, one gets the output [1 / 4 + 5, 3 / 4 + 6], i.e. [5.25, 6.75]\n\n\n\n\n\nmean(*columns)\nGet the mean value.\nThis function is syntactic sugar for pl.col(columns).mean().\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n*columns\nstr\nOne or more column names.\n()\n\n\n\n\n\n\nmean_horizontal\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 8, 3],\n...         \"b\": [4, 5, 2],\n...         \"c\": [\"foo\", \"bar\", \"foo\"],\n...     }\n... )\n&gt;&gt;&gt; df.select(pl.mean(\"a\"))\nshape: (1, 1)\n┌─────┐\n│ a   │\n│ --- │\n│ f64 │\n╞═════╡\n│ 4.0 │\n└─────┘\n&gt;&gt;&gt; df.select(pl.mean(\"a\", \"b\"))\nshape: (1, 2)\n┌─────┬──────────┐\n│ a   ┆ b        │\n│ --- ┆ ---      │\n│ f64 ┆ f64      │\n╞═════╪══════════╡\n│ 4.0 ┆ 3.666667 │\n└─────┴──────────┘\n\n\n\n\nmedian(*columns)\nGet the median value.\nThis function is syntactic sugar for pl.col(columns).median().\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncolumns\nstr\nOne or more column names.\n()\n\n\n\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 8, 3],\n...         \"b\": [4, 5, 2],\n...         \"c\": [\"foo\", \"bar\", \"foo\"],\n...     }\n... )\n&gt;&gt;&gt; df.select(pl.median(\"a\"))\nshape: (1, 1)\n┌─────┐\n│ a   │\n│ --- │\n│ f64 │\n╞═════╡\n│ 3.0 │\n└─────┘\n&gt;&gt;&gt; df.select(pl.median(\"a\", \"b\"))\nshape: (1, 2)\n┌─────┬─────┐\n│ a   ┆ b   │\n│ --- ┆ --- │\n│ f64 ┆ f64 │\n╞═════╪═════╡\n│ 3.0 ┆ 4.0 │\n└─────┴─────┘\n\n\n\n\nn_unique(*columns)\nCount unique values.\nThis function is syntactic sugar for pl.col(columns).n_unique().\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncolumns\nstr\nOne or more column names.\n()\n\n\n\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 8, 1],\n...         \"b\": [4, 5, 2],\n...         \"c\": [\"foo\", \"bar\", \"foo\"],\n...     }\n... )\n&gt;&gt;&gt; df.select(pl.n_unique(\"a\"))\nshape: (1, 1)\n┌─────┐\n│ a   │\n│ --- │\n│ u32 │\n╞═════╡\n│ 2   │\n└─────┘\n&gt;&gt;&gt; df.select(pl.n_unique(\"b\", \"c\"))\nshape: (1, 2)\n┌─────┬─────┐\n│ b   ┆ c   │\n│ --- ┆ --- │\n│ u32 ┆ u32 │\n╞═════╪═════╡\n│ 3   ┆ 2   │\n└─────┴─────┘\n\n\n\n\nnth(*indices)\nGet the nth column(s) of the context.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nindices\nint | Sequence[int]\nOne or more indices representing the columns to retrieve.\n()\n\n\n\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 8, 3],\n...         \"b\": [4, 5, 2],\n...         \"c\": [\"foo\", \"bar\", \"baz\"],\n...     }\n... )\n&gt;&gt;&gt; df.select(pl.nth(1))\nshape: (3, 1)\n┌─────┐\n│ b   │\n│ --- │\n│ i64 │\n╞═════╡\n│ 4   │\n│ 5   │\n│ 2   │\n└─────┘\n&gt;&gt;&gt; df.select(pl.nth(2, 0))\nshape: (3, 2)\n┌─────┬─────┐\n│ c   ┆ a   │\n│ --- ┆ --- │\n│ str ┆ i64 │\n╞═════╪═════╡\n│ foo ┆ 1   │\n│ bar ┆ 8   │\n│ baz ┆ 3   │\n└─────┴─────┘\n\n\n\n\nquantile(column, quantile, interpolation='nearest')\nSyntactic sugar for pl.col(\"foo\").quantile(..).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncolumn\nstr\nColumn name.\nrequired\n\n\nquantile\nfloat | Expr\nQuantile between 0.0 and 1.0.\nrequired\n\n\ninterpolation\n(‘nearest’, ‘higher’, ‘lower’, ‘midpoint’, ‘linear’)\nInterpolation method.\n'nearest'\n\n\n\n\n\n\n\nreduce(function, exprs)\nAccumulate over multiple columns horizontally/ row wise with a left fold.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunction\nCallable[[Series, Series], Series]\nFunction to apply over the accumulator and the value. Fn(acc, value) -&gt; new_value\nrequired\n\n\nexprs\nSequence[Expr | str] | Expr\nExpressions to aggregate over. May also be a wildcard expression.\nrequired\n\n\n\n\n\n\nSee fold for the version with an explicit accumulator.\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 2, 3],\n...         \"b\": [0, 1, 2],\n...     }\n... )\n&gt;&gt;&gt; df\nshape: (3, 2)\n┌─────┬─────┐\n│ a   ┆ b   │\n│ --- ┆ --- │\n│ i64 ┆ i64 │\n╞═════╪═════╡\n│ 1   ┆ 0   │\n│ 2   ┆ 1   │\n│ 3   ┆ 2   │\n└─────┴─────┘\nHorizontally sum over all columns.\n&gt;&gt;&gt; df.select(\n...     pl.reduce(function=lambda acc, x: acc + x, exprs=pl.col(\"*\")).alias(\"sum\")\n... )\nshape: (3, 1)\n┌─────┐\n│ sum │\n│ --- │\n│ i64 │\n╞═════╡\n│ 1   │\n│ 3   │\n│ 5   │\n└─────┘\n\n\n\n\nrolling_corr(a, b, *, window_size, min_periods=None, ddof=1)\nCompute the rolling correlation between two columns/ expressions.\n.. warning:: This functionality is considered unstable. It may be changed at any point without it being considered a breaking change.\nThe window at a given row includes the row itself and the window_size - 1 elements before it.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\na\nstr | Expr\nColumn name or Expression.\nrequired\n\n\nb\nstr | Expr\nColumn name or Expression.\nrequired\n\n\nwindow_size\nint\nThe length of the window.\nrequired\n\n\nmin_periods\nint | None\nThe number of values in the window that should be non-null before computing a result. If None, it will be set equal to window size.\nNone\n\n\nddof\nint\nDelta degrees of freedom. The divisor used in calculations is N - ddof, where N represents the number of elements.\n1\n\n\n\n\n\n\n\nrolling_cov(a, b, *, window_size, min_periods=None, ddof=1)\nCompute the rolling covariance between two columns/ expressions.\n.. warning:: This functionality is considered unstable. It may be changed at any point without it being considered a breaking change.\nThe window at a given row includes the row itself and the window_size - 1 elements before it.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\na\nstr | Expr\nColumn name or Expression.\nrequired\n\n\nb\nstr | Expr\nColumn name or Expression.\nrequired\n\n\nwindow_size\nint\nThe length of the window.\nrequired\n\n\nmin_periods\nint | None\nThe number of values in the window that should be non-null before computing a result. If None, it will be set equal to window size.\nNone\n\n\nddof\nint\nDelta degrees of freedom. The divisor used in calculations is N - ddof, where N represents the number of elements.\n1\n\n\n\n\n\n\n\nselect(*exprs, **named_exprs)\nRun polars expressions without a context.\nThis is syntactic sugar for running df.select on an empty DataFrame.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n*exprs\nIntoExpr | Iterable[IntoExpr]\nColumn(s) to select, specified as positional arguments. Accepts expression input. Strings are parsed as column names, other non-expression inputs are parsed as literals.\n()\n\n\n**named_exprs\nIntoExpr\nAdditional columns to select, specified as keyword arguments. The columns will be renamed to the keyword used.\n{}\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nDataFrame\n\n\n\n\n\n\n\n&gt;&gt;&gt; foo = pl.Series(\"foo\", [1, 2, 3])\n&gt;&gt;&gt; bar = pl.Series(\"bar\", [3, 2, 1])\n&gt;&gt;&gt; pl.select(min=pl.min_horizontal(foo, bar))\nshape: (3, 1)\n┌─────┐\n│ min │\n│ --- │\n│ i64 │\n╞═════╡\n│ 1   │\n│ 2   │\n│ 1   │\n└─────┘\n\n\n\n\nsql_expr(sql)\nParse one or more SQL expressions to Polars expression(s).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsql\nstr | Sequence[str]\nOne or more SQL expressions.\nrequired\n\n\n\n\n\n\nParse a single SQL expression:\n&gt;&gt;&gt; df = pl.DataFrame({\"a\": [2, 1]})\n&gt;&gt;&gt; expr = pl.sql_expr(\"MAX(a)\")\n&gt;&gt;&gt; df.select(expr)\nshape: (1, 1)\n┌─────┐\n│ a   │\n│ --- │\n│ i64 │\n╞═════╡\n│ 2   │\n└─────┘\nParse multiple SQL expressions:\n&gt;&gt;&gt; df.with_columns(\n...     *pl.sql_expr([\"POWER(a,a) AS a_a\", \"CAST(a AS TEXT) AS a_txt\"]),\n... )\nshape: (2, 3)\n┌─────┬─────┬───────┐\n│ a   ┆ a_a ┆ a_txt │\n│ --- ┆ --- ┆ ---   │\n│ i64 ┆ i64 ┆ str   │\n╞═════╪═════╪═══════╡\n│ 2   ┆ 4   ┆ 2     │\n│ 1   ┆ 1   ┆ 1     │\n└─────┴─────┴───────┘\n\n\n\n\nstd(column, ddof=1)\nGet the standard deviation.\nThis function is syntactic sugar for pl.col(column).std(ddof).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncolumn\nstr\nColumn name.\nrequired\n\n\nddof\nint\n“Delta Degrees of Freedom”: the divisor used in the calculation is N - ddof, where N represents the number of elements. By default ddof is 1.\n1\n\n\n\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 8, 3],\n...         \"b\": [4, 5, 2],\n...         \"c\": [\"foo\", \"bar\", \"foo\"],\n...     }\n... )\n&gt;&gt;&gt; df.select(pl.std(\"a\"))\nshape: (1, 1)\n┌──────────┐\n│ a        │\n│ ---      │\n│ f64      │\n╞══════════╡\n│ 3.605551 │\n└──────────┘\n&gt;&gt;&gt; df[\"a\"].std()\n3.605551275463989\n\n\n\n\ntail(column, n=10)\nGet the last n rows.\nThis function is syntactic sugar for pl.col(column).tail(n).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncolumn\nstr\nColumn name.\nrequired\n\n\nn\nint\nNumber of rows to return.\n10\n\n\n\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 8, 3],\n...         \"b\": [4, 5, 2],\n...         \"c\": [\"foo\", \"bar\", \"foo\"],\n...     }\n... )\n&gt;&gt;&gt; df.select(pl.tail(\"a\"))\nshape: (3, 1)\n┌─────┐\n│ a   │\n│ --- │\n│ i64 │\n╞═════╡\n│ 1   │\n│ 8   │\n│ 3   │\n└─────┘\n&gt;&gt;&gt; df.select(pl.tail(\"a\", 2))\nshape: (2, 1)\n┌─────┐\n│ a   │\n│ --- │\n│ i64 │\n╞═════╡\n│ 8   │\n│ 3   │\n└─────┘\n\n\n\n\nvar(column, ddof=1)\nGet the variance.\nThis function is syntactic sugar for pl.col(column).var(ddof).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncolumn\nstr\nColumn name.\nrequired\n\n\nddof\nint\n“Delta Degrees of Freedom”: the divisor used in the calculation is N - ddof, where N represents the number of elements. By default ddof is 1.\n1\n\n\n\n\n\n\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"a\": [1, 8, 3],\n...         \"b\": [4, 5, 2],\n...         \"c\": [\"foo\", \"bar\", \"foo\"],\n...     },\n... )\n&gt;&gt;&gt; df.select(pl.var(\"a\"))\nshape: (1, 1)\n┌──────┐\n│ a    │\n│ ---  │\n│ f64  │\n╞══════╡\n│ 13.0 │\n└──────┘\n&gt;&gt;&gt; df[\"a\"].var()\n13.0",
    "crumbs": [
      "Reference",
      "Main Functions",
      "functions"
    ]
  }
]